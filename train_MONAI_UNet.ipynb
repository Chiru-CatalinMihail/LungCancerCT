{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### MACROS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "WEIGHT_DECAY = 'weight_decay'\n",
        "LEARNING_RATE = 'lr'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### IMPORTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install albumentations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\") # remove some scikit-image warnings\n",
        "\n",
        "# import monai\n",
        "# # monai.config.print_config()\n",
        "\n",
        "from monai.apps import DecathlonDataset\n",
        "from monai.data import DataLoader, CacheDataset, decollate_batch\n",
        "# # from monai.data import decollate_patient_batch\n",
        "# from monai.utils import first, set_determinism\n",
        "from monai.networks.nets import UNet, DynUNet, AttentionUnet, ViTAutoEnc, UNETR\n",
        "from monai.networks.layers import Norm\n",
        "from monai.metrics import DiceMetric, MeanIoU, compute_average_surface_distance, DiceHelper\n",
        "from monai.losses import DiceLoss\n",
        "from monai.inferers import sliding_window_inference\n",
        "# import albumentations as A\n",
        "from monai.transforms import (\n",
        "    LoadImage,\n",
        "    LoadImageD,\n",
        "    EnsureChannelFirstD,\n",
        "#     AddChannelD,\n",
        "    ScaleIntensityD,\n",
        "    ToTensorD,\n",
        "    Compose,\n",
        "    AsDiscreteD,\n",
        "    SpacingD,\n",
        "    OrientationD,\n",
        "    ResizeD,\n",
        "    RandAffineD,\n",
        "    AsDiscrete,\n",
        "    AsDiscreted,\n",
        "    EnsureTyped,\n",
        "    EnsureType,\n",
        "    LoadImageD,\n",
        "    EnsureChannelFirstD,\n",
        "    OrientationD,\n",
        "    SpacingD,\n",
        "    ScaleIntensity,\n",
        "    ResizeD,\n",
        "    RandAffineD,\n",
        "    RandFlipD,\n",
        "    RandRotateD,\n",
        "    RandZoomD,\n",
        "#     RandDeformD,\n",
        "    ToTensorD,\n",
        "    Activations, AsDiscrete\n",
        ")\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader as TorchDataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import cv2\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import pickle as pkl\n",
        "\n",
        "from torch.utils.data.dataloader import default_collate\n",
        "\n",
        "from hyperparams import *\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### HYPERPARAMS ###\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "### HYPERPARAMS ###\n",
        "crt_dir = os.getcwd()\n",
        "# datasets_path = f'/raid/CataChiru/MedicalDecathlonTensors/'\n",
        "# datasets_path = f'./datasets/MedicalDecathlonJustLungs/'\n",
        "# datasets_path = f'./datasets/MedicalDecathlonJustTumors/'\n",
        "# datasets_path = f'./datasets/MedicalDecathlonAugmentedTumors/'\n",
        "datasets_path = f'./datasets/MedicalDecathlonAugmentedTumors2/'\n",
        "# datasets_path = f'./datasets/MedicalDecathlonClaheTumors/'\n",
        "\n",
        "model_name = 'unet'\n",
        "checkpoints_path = f'{crt_dir}/checkpoints/{model_name.upper()}/'\n",
        "\n",
        "DEBUG_MODE = True\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### MAIN ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You are using cuda\n",
            "Number of images in a stack: 64\n"
          ]
        }
      ],
      "source": [
        "# Initialize torch and cuda\n",
        "cuda = torch.cuda.is_available()\n",
        "\n",
        "# Set device to 3rd GPU\n",
        "# device = torch.device(\"cuda:2\" if cuda else \"cpu\")\n",
        "\n",
        "# Use all GPUs\n",
        "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
        "\n",
        "num_workers = 4 if cuda else 1\n",
        "\n",
        "print(f'You are using {device}')\n",
        "\n",
        "print(f'Number of images in a stack: {NO_STACKED_IMGS}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Used to bypass this error: https://github.com/pytorch/pytorch/issues/113245\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3,2,1,0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# torch.multiprocessing.set_start_method('spawn', force=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set random seed\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "random.seed(0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DEBUG FUNCTIONALITIES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def debug_plot(img, mask):\n",
        "    '''\n",
        "    Plots the image and mask as two subplots, for how many stacked images there are\n",
        "    '''\n",
        "\n",
        "    # 6x2 subplots, with reduced vertical space\n",
        "    \n",
        "    fig, axes = plt.subplots(img.shape[-1], 2, figsize=(10, 10))\n",
        "\n",
        "\n",
        "    for i in range(img.shape[-1]):\n",
        "        axes[i, 0].invert_yaxis()\n",
        "        axes[i, 1].invert_yaxis()\n",
        "        axes[i, 0].imshow(img[0, ..., i], cmap='gray')\n",
        "        axes[i, 1].imshow(img[0, ..., i], cmap='gray')\n",
        "        axes[i, 1].imshow(mask[0, ..., i], alpha=0.5, cmap='jet')\n",
        "        axes[i, 0].axis('off')\n",
        "        axes[i, 1].axis('off')\n",
        "\n",
        "    plt.subplots_adjust(wspace=0)    \n",
        "    plt.show()\n",
        "\n",
        "def debug_plot_single(img, mask):\n",
        "    a = img[0].to('cpu').detach().numpy()\n",
        "    b = mask[0].to('cpu').detach().numpy()\n",
        "    for i in range(img.shape[-1]):\n",
        "        plt.imshow(a[0, ..., i], cmap = 'gray')\n",
        "        plt.imshow(b[0, ..., i], cmap = 'jet', alpha = 0.5)\n",
        "        plt.gca().invert_yaxis()\n",
        "        # plt.gca().set_aspect('equal', adjustable='box')\n",
        "        plt.gca().set_axis_off()\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### PREPROCESSING TRANSFORMS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Used Dictionary transforms from MONAI to apply the same transforms to both the image and the label\n",
        "train_transform = Compose([RandRotateD(range_x=0, range_y=0, range_z=np.pi/12, prob=1, keys=['image', 'label'])])\n",
        "post_pred = Compose([AsDiscrete(argmax=True, to_onehot= 2)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DATA LOADER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_tumour_percentage_per_patient(tumour):\n",
        "    '''\n",
        "    Computes the percentage of tumour in each patient\n",
        "    '''\n",
        "\n",
        "    return 100*np.sum(tumour) / np.prod(tumour.shape)\n",
        "\n",
        "def build_stack_ordered_indices(split_type : str, slices_per_patient: [int], stack_size : int, tumours : list = None, upload_flag : bool = True ) -> list[tuple]:\n",
        "    ''' Iterates an overlapping sliding window of stack_size images for each patient per batch.\n",
        "\n",
        "    For split_type = 'training', the stacks are with an overlapping window from start to finish.\n",
        "    Returns a list of tuples (patient_id, stack_indices, tumour_percentage, has_tumour, original_idx) \n",
        "    \n",
        "    For split_type = 'validation', the stacks are built with a stride of \"stack_size\" over the entire volume of each patient, and the last stack is padded with the last slice of the volume up to \"stack_size\" slices.\n",
        "    Returns a list of tuples (patient_id, stack_indices, tumour_percentage, has_tumour, original_idx) \n",
        "\n",
        "\n",
        "    '''\n",
        "    stacks_in_order_indices =[]\n",
        "    if split_type == \"training\":\n",
        "        real_idx = 0\n",
        "        # Saves the indices of the sliding window for each patient\n",
        "        for patient_id, slices in tqdm(enumerate(slices_per_patient)):\n",
        "            crt_slices = []\n",
        "\n",
        "            for i in range(0, slices - stack_size + 1):\n",
        "                    stacks_range = np.arange(i, i+stack_size)\n",
        "                    crt_tumour = tumours[patient_id][1][..., i:i+stack_size]\n",
        "                    tumour_percentage = compute_tumour_percentage_per_patient(crt_tumour)\n",
        "\n",
        "                    crt_slices.append((patient_id, stacks_range, tumour_percentage, tumour_percentage > 0, real_idx))\n",
        "\n",
        "                    real_idx += 1\n",
        "            \n",
        "            \n",
        "            stacks_in_order_indices += crt_slices\n",
        "\n",
        "    elif split_type == 'validation':\n",
        "\n",
        "        for patient_id, slices in enumerate(slices_per_patient):\n",
        "            # Non-overlapping sliding window of stack_size images, with stride = stack_size\n",
        "\n",
        "            padding = stack_size - slices % stack_size\n",
        "            remaining_difference = -1\n",
        "\n",
        "            for i in range(0, slices, stack_size):\n",
        "                remaining_difference = i + stack_size - slices\n",
        "                if remaining_difference > 0:\n",
        "                    break\n",
        "\n",
        "                stacks_in_order_indices.append((patient_id, np.arange(i, i+stack_size)))\n",
        "\n",
        "            # If the last stack is smaller than stack_size, we pad it with the last slice of the volume\n",
        "            if padding  % stack_size != 0 and remaining_difference > 0:\n",
        "                remaining_slices_indices = np.arange(i, slices)\n",
        "                repeated_slices = np.repeat(slices - 1, padding)\n",
        "                batch_indices = np.hstack((remaining_slices_indices, repeated_slices))\n",
        "                stacks_in_order_indices.append((patient_id, batch_indices))\n",
        "\n",
        "\n",
        "    if upload_flag and not os.path.exists(f'./ordered_{split_type}_indices_stack={stack_size}.pkl'):\n",
        "        with open(f'./ordered_{split_type}_indices_stack={stack_size}.pkl', 'wb') as f:\n",
        "            pkl.dump(stacks_in_order_indices, f)\n",
        "\n",
        "    return stacks_in_order_indices\n",
        "\n",
        "\n",
        "def create_oversampled_index_dataset(ordered_stacks, split_type, stack_size, print_flag = False, \n",
        "    tumour_percent_threshold : float = 0.5, tumorous_proportion : float = 0.7, upload_flag : bool = True,\n",
        "    undersample_flag : bool = False, undersample_size : int = 300):\n",
        "    '''\n",
        "    Based on the threshold set for the tumour percentage, splits the dataset and oversamples the desired portion of the dataset\n",
        "    Returns the complete list of indices by intercalating the two portions\n",
        "    '''\n",
        "\n",
        "    ordered_stacks.sort(key = lambda x: x[2], reverse = True)\n",
        "\n",
        "    small_tumour_stacks = list(filter(lambda x: x[2] < tumour_percent_threshold, ordered_stacks))\n",
        "\n",
        "    print(\"Small tumour stacks\", len(small_tumour_stacks))\n",
        "\n",
        "    # TODO shuffle tensor and keep only undersample_size\n",
        "    if undersample_flag:\n",
        "        np.random.shuffle(small_tumour_stacks)\n",
        "        if undersample_size < len(small_tumour_stacks):\n",
        "            small_tumour_stacks = small_tumour_stacks[:undersample_size]\n",
        "\n",
        "    length_small_tumour_stacks = len(small_tumour_stacks)\n",
        "\n",
        "    print(\"Small tumour stacks2\", length_small_tumour_stacks)\n",
        "\n",
        "\n",
        "    big_tumour_stacks = list(filter(lambda x: x[2] > tumour_percent_threshold, ordered_stacks))\n",
        "\n",
        "    print(\"Big tumour stacks\", len(big_tumour_stacks))\n",
        "\n",
        "    length_big_tumour_stacks = len(big_tumour_stacks)\n",
        "\n",
        "    oversampling_factor = int(length_small_tumour_stacks / ((1-tumorous_proportion) * length_big_tumour_stacks))\n",
        "    if length_big_tumour_stacks < length_small_tumour_stacks and oversampling_factor > 1:\n",
        "        big_tumour_stacks = big_tumour_stacks * oversampling_factor\n",
        "\n",
        "    print(\"Small tumour stacks3\", length_small_tumour_stacks)\n",
        "    print(\"Big tumour stacks2\", length_big_tumour_stacks)\n",
        "\n",
        "    length = length_small_tumour_stacks + len(big_tumour_stacks)\n",
        "\n",
        "    all_stacks = []\n",
        "\n",
        "    small_tumour_idx = 0\n",
        "    big_tumour_idx = 0\n",
        "    \n",
        "    for i in range(length):\n",
        "        if i % 10 >= 7 and small_tumour_idx < length_small_tumour_stacks:\n",
        "            all_stacks.append(small_tumour_stacks[small_tumour_idx])\n",
        "            small_tumour_idx += 1\n",
        "\n",
        "            if print_flag:\n",
        "                print(\"Small\", i, small_tumour_idx)\n",
        "        else:\n",
        "            if big_tumour_idx >= len(big_tumour_stacks):\n",
        "                break\n",
        "\n",
        "            all_stacks.append(big_tumour_stacks[big_tumour_idx])\n",
        "            big_tumour_idx += 1\n",
        "            if print_flag:\n",
        "                print(\"Big\", i, big_tumour_idx)\n",
        "\n",
        "    if upload_flag and not os.path.exists(f'./{split_type}_indices_stack={stack_size}.pkl'):\n",
        "        with open(f'./{split_type}_indices_stack=6.pkl', 'wb') as f:\n",
        "            pkl.dump(all_stacks, f)\n",
        "\n",
        "    np.random.shuffle(all_stacks)\n",
        "\n",
        "    print(\"Length of the dataset\", len(all_stacks))\n",
        "    return all_stacks\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def convert_labels_to_one_hot(labels: torch.Tensor, num_classes: int) -> torch.Tensor:\n",
        "    ''' Converts a tensor of labels to a one-hot tensor in which each channel corresponds to a binary decision for each class from the original tensor.'''\n",
        "    \n",
        "    one_hot = torch.zeros((2*labels.shape[0], labels.shape[1], labels.shape[2], labels.shape[3])).to(labels.device)\n",
        "    \n",
        "    one_hot[0, :, :, :] = (labels == 0).squeeze(1).float()\n",
        "    one_hot[1, :, :, :] = (labels != 0).squeeze(1).float()\n",
        "\n",
        "    return one_hot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "class msdDataset(Dataset):\n",
        "    def __init__(self, dataset_folder, transform = None, stack_size = 6, tumour_percent_threshold = 0.5, tumorous_proportion = 0.7, undersample_flag = False, undersample_size = 300):\n",
        "        '''Am stabilit stack_size la 6 pe baza discutiei cu Doamna Udrea care sugera intre 3 si 6 imagini in stack + EDA2\n",
        "        \n",
        "        \n",
        "        self.patients - contains (image, label) pairs for each patient in the dataset\n",
        "        self.stacks_in_order_indices - contains the tuples (patient_id, stack_indices, other relevant attributes based on split_type) for each stack in the dataset\n",
        "        \n",
        "        '''\n",
        "        self.img_folder = dataset_folder + \"images/\"\n",
        "        self.label_folder = dataset_folder + \"labels/\"\n",
        "        self.no_patients = len(os.listdir(self.img_folder))\n",
        "        self.stack_size = stack_size\n",
        "        self.transform = transform\n",
        "\n",
        "        split_type = 'training' if 'training' in dataset_folder else 'validation'\n",
        "\n",
        "        # Flag that indicates we are working with the training dataset, we want to apply random rotations only for this split\n",
        "        self.train_flag = split_type == 'training'\n",
        "\n",
        "        print(split_type)\n",
        "        self.patients = [self.get_img_and_label(i) for i in range(self.no_patients)]\n",
        "\n",
        "        # If the indices for the dataset have been already built, load them, otherwise build them\n",
        "        if os.path.exists(f'./ordered_{split_type}_indices_stack={stack_size}.pkl'):\n",
        "            print(f'./ordered_{split_type}_indices_stack={stack_size}.pkl exists. Loading the ordered indices.')\n",
        "            with open(f'./ordered_{split_type}_indices_stack={stack_size}.pkl', 'rb') as f:\n",
        "                self.stacks_in_order_indices = pkl.load(f)\n",
        "        else:\n",
        "            print(f'./ordered_{split_type}_indices_stack={stack_size}.pkl does not exist. Building the ordered indices.')\n",
        "            slices_per_patient = [self.patients[i][0].shape[-1] for i in range(self.no_patients)]\n",
        "            self.stacks_in_order_indices = build_stack_ordered_indices(split_type, slices_per_patient, stack_size, self.patients, upload_flag = True)\n",
        "            \n",
        "        if split_type == 'training':\n",
        "            if os.path.exists(f'./{split_type}_indices_stack={stack_size}.pkl'):\n",
        "                print(f'./{split_type}_indices_stack={stack_size}.pkl exists. Loading the overall indices.')\n",
        "                with open(f'./{split_type}_indices_stack={stack_size}.pkl', 'rb') as f:\n",
        "                    self.stacks_in_order_indices = pkl.load(f)\n",
        "            else:\n",
        "                print(f'./{split_type}_indices_stack={stack_size}.pkl does not exist. Building the overall indices.')\n",
        "                self.stacks_in_order_indices = create_oversampled_index_dataset(self.stacks_in_order_indices, split_type, stack_size, \n",
        "                                                                                tumour_percent_threshold = tumour_percent_threshold, tumorous_proportion = tumorous_proportion,\n",
        "                                                                                undersample_flag=undersample_flag, undersample_size = undersample_size)\n",
        "        \n",
        "\n",
        "        # self.device = device\n",
        "        self.length = len(self.stacks_in_order_indices)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def get_img_and_label(self, patient_id):\n",
        "        ''' Helper function: For a specified patient returns its image and label stacks from the dataset '''\n",
        "\n",
        "        img = torch.load(self.img_folder + f'patient_{patient_id}.pt')\n",
        "        label = torch.load(self.label_folder + f'patient_{patient_id}.pt')\n",
        "        return img, label\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # print(f'Getting item {idx}/{len(self)}')\n",
        "        # print(self.img_folder + f'patient_{patient_id}.pt')\n",
        "\n",
        "        # Based on current index, get the patient_id and the slices that form the current stack\n",
        "\n",
        "        if idx >= 0 and idx < self.length:\n",
        "            stacks_tuple = self.stacks_in_order_indices[idx]\n",
        "            patient_id, chosen_stacks = stacks_tuple[0], stacks_tuple[1]\n",
        "\n",
        "            img, label = self.patients[patient_id]\n",
        "            # Filters the current stack of images and labels for the current batch\n",
        "            img, label = img[..., chosen_stacks], label[..., chosen_stacks]\n",
        "\n",
        "            if self.train_flag and self.transform and stacks_tuple[3]:\n",
        "                output = self.transform({'image': img, 'label': label})\n",
        "                img, label = output['image'], output['label']\n",
        "\n",
        "            # label = convert_labels_to_one_hot(label, 2) # Not needed for now, as MONAI handles the conversion internally\n",
        "            return img, label\n",
        "        else:\n",
        "            raise IndexError\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_name = \"training/\"\n",
        "val_name = \"validation/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'./datasets/MedicalDecathlonAugmentedTumors2/'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "datasets_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('Batch size: ', 16)"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"Batch size: \", BATCH_SIZE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "msd_train_dataset = msdDataset(datasets_path + train_name, transform = train_transform, tumour_percent_threshold=0.125, tumorous_proportion=0.4, undersample_flag=True, undersample_size=1200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(msd_train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "img, label = msd_train_dataset[0]\n",
        "\n",
        "print(img.shape, label.shape)\n",
        "\n",
        "output = train_transform({'image': img, 'label': label})\n",
        "\n",
        "img_transformed, label_transformed = output['image'], output['label']\n",
        "\n",
        "debug_plot(img_transformed, label_transformed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "validation\n",
            "./ordered_validation_indices_stack=6.pkl exists. Loading the ordered indices.\n"
          ]
        }
      ],
      "source": [
        "msd_val_dataset = msdDataset(datasets_path + val_name, transform = None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'msd_train_dataset' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_355001/1013125836.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsd_train_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsd_val_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'msd_train_dataset' is not defined"
          ]
        }
      ],
      "source": [
        "len(msd_train_dataset), len(msd_val_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_loader = TorchDataLoader(msd_train_dataset, batch_size = BATCH_SIZE, shuffle = True, collate_fn=lambda x: tuple(x_ for x_ in default_collate(x))) # TODO: Comment collate_fn if it doesn't work"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "val_loader = TorchDataLoader(msd_val_dataset, batch_size = BATCH_SIZE, shuffle = False, collate_fn=lambda x: tuple(x_ for x_ in default_collate(x))) #  num_workers = num_workers,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for a in tqdm(train_loader):\n",
        "    # print(a[0].shape, a[1].shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "debug_img, debug_label = next(iter(train_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "debug_img.shape, debug_label.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_channels = debug_img.shape[1]\n",
        "output_channels = 2*input_channels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "debug_img, debug_label = debug_img.to(device), debug_label.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "debug_plot_single(debug_img, debug_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "debug_img_val, debug_label_val = next(iter(val_loader))\n",
        "\n",
        "debug_plot_single(debug_img_val, debug_label_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MODEL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### MODEL HYPERPARAMS ###\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 100 EPOCHS:\n",
        "# UNet_metadata = dict(\n",
        "#     spatial_dims = 3,\n",
        "#     in_channels = 1,\n",
        "#     out_channels = 2,\n",
        "#     channels = (16, 32, 32, 128),\n",
        "#     kernel_size = (5, 5, 5),\n",
        "#     strides = (1, 2, 1, 2),\n",
        "#     num_res_units = 4,\n",
        "#     norm = Norm.BATCH,\n",
        "#     # act = torch.nn.ReLU,\n",
        "#     dropout = 0.1\n",
        "# )\n",
        "\n",
        "UNet_metadata = dict(\n",
        "    spatial_dims = 3,\n",
        "    in_channels = 1,\n",
        "    out_channels = 2,\n",
        "    channels = (16, 32, 32, 128),\n",
        "    kernel_size = (5, 5, 5),\n",
        "    strides = (1, 2, 1, 2),\n",
        "    num_res_units = 4,\n",
        "    norm = Norm.BATCH,\n",
        "    # act = torch.nn.ReLU,\n",
        "    dropout = 0.1\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_previous_model_name = './checkpoints/UNET/min-max5_augmented_unet_nadam_lr5.00e-03_wd1.00e-04_diceloss_epoch94.pth'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Instantiate model\n",
        "net = nn.Sequential(nn.BatchNorm3d(1, affine=True), UNet(**UNet_metadata), nn.InstanceNorm3d(2, affine=True))\n",
        "\n",
        "net = nn.DataParallel(net, device_ids = [0, 1, 2, 3])\n",
        "net = net.to(device)\n",
        "# net.load_state_dict(torch.load(best_previous_model_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# See how much gpu the model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "debug_img.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "net(debug_img).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### LOSS ###\n",
        "loss_functions = {\n",
        "    'dice': DiceLoss(to_onehot_y = True, softmax = True, include_background=False),\n",
        "    'cross_entropy': nn.CrossEntropyLoss(),\n",
        "    'custom': nn.BCELoss()\n",
        "}\n",
        "\n",
        "loss_key = 'dice'\n",
        "\n",
        "loss_function = loss_functions[loss_key]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "LEARNING_RATE = 5e-3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "optimizers = { 'adam' : torch.optim.Adam, 'sgd' : torch.optim.SGD, 'nadam' : torch.optim.NAdam, 'rmsprop' : torch.optim.RMSprop, 'adamw' : torch.optim.AdamW}\n",
        "optimizer_key = 'nadam'\n",
        "\n",
        "# Instantiate optimizer\n",
        "decayRate = 1  #- 1e-3\n",
        "optimizer = optimizers[optimizer_key](net.parameters(), lr = LEARNING_RATE, weight_decay = 1e-4)\n",
        "\n",
        "# Instantiate learning rate scheduler\n",
        "lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decayRate)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### PERFORMANCE METRICS ###\n",
        "dice_metric = DiceHelper(include_background = False, reduction = \"mean\", get_not_nans=False, ignore_empty=True) # include_background = False,\n",
        "iou_metric = MeanIoU(include_background=False, reduction = \"mean\", get_not_nans=False, ignore_empty=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TRAINING PROCEDURE ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(device, model, model_name, train_legth, train_loader, val_loader, loss_function, optimizer, lr_scheduler, MAX_EPOCHS=20, VALIDATION_INTERVAL=2, EPOCH_OFFSET : int = 0):\n",
        "\n",
        "    lr_val = optimizer.defaults['lr']\n",
        "    wd_val = optimizer.defaults[WEIGHT_DECAY]\n",
        "\n",
        "    # Variables to get the best model\n",
        "    best_dice = -1\n",
        "    best_metrics = None\n",
        "    best_metric_epoch = -1\n",
        "\n",
        "    general_name = f'min-max5_augmented_{model_name}_{optimizer_key}_lr{lr_val:.2e}_wd{wd_val:.2e}_{loss_key}loss'\n",
        "    best_model_name = checkpoints_path + f'{general_name}_best.pth'\n",
        "\n",
        "    print(best_model_name)\n",
        "    writer = SummaryWriter(log_dir=f\"./pytorch_logging/{general_name}_epochs{MAX_EPOCHS}\")\n",
        "\n",
        "\n",
        "    # Evaluation metrics per epoch\n",
        "    dice_values = []\n",
        "    iou_values = []\n",
        "\n",
        "    epoch_loss_values = []\n",
        "\n",
        "    for epoch in range(1 + EPOCH_OFFSET, MAX_EPOCHS + 1):\n",
        "        print(\"-\" * 12)\n",
        "        print(f\"Epoch {epoch}/{MAX_EPOCHS}\")\n",
        "\n",
        "        # Turn model to \"train\" mode\n",
        "        model.train()\n",
        "\n",
        "        epoch_loss = 0\n",
        "        for step, batch_data in enumerate(train_loader):\n",
        "            step += 1\n",
        "\n",
        "            train_input, label = batch_data\n",
        "            train_input, label = train_input.to(device), label.to(device)\n",
        "\n",
        "            output = model(train_input)\n",
        "\n",
        "            loss = loss_function(output, label)\n",
        "            loss.backward() # Compute gradient\n",
        "            optimizer.step() # Update model's parameters\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            print(f\"{step}/{train_legth // train_loader.batch_size}, \"\n",
        "                f\"train_loss: {loss.item():.4f}\")\n",
        "\n",
        "        epoch_loss /= step\n",
        "        epoch_loss_values.append(epoch_loss)\n",
        "        writer.add_scalar('Loss/train', epoch_loss, epoch)\n",
        "\n",
        "        print(f\"epoch {epoch} average loss: {epoch_loss:.4f}\")\n",
        "\n",
        "        if epoch % VALIDATION_INTERVAL == 0:\n",
        "            # Save current checkpoint of the network\n",
        "\n",
        "            print(f\"Saving checkpoint: {epoch//VALIDATION_INTERVAL + 1} / {MAX_EPOCHS//VALIDATION_INTERVAL}!!!\")\n",
        "            name = checkpoints_path + f'{general_name}_epoch{epoch}.pth'\n",
        "            torch.save(model.state_dict(), name)\n",
        "\n",
        "\n",
        "            # Decay learning rate\n",
        "            lr_scheduler.step()\n",
        "\n",
        "            # Turn model to \"eval\" mode\n",
        "            model.eval()\n",
        "\n",
        "\n",
        "            # Disabling gradient calculation is useful for inference, when you are sure that you will not call Tensor.backward().\n",
        "            # It will reduce memory consumption for computations that would otherwise have requires_grad=True\n",
        "            with torch.no_grad():\n",
        "                iteration_dice = []\n",
        "                # iteration_ious = []\n",
        "                # iteration_pixel_accuracies = []\n",
        "                # iteration_rvds = []\n",
        "\n",
        "                for val_data in val_loader:\n",
        "                    val_input, val_label = val_data\n",
        "                    val_input, val_label = val_input.to(device), val_label.to(device)\n",
        "\n",
        "                    val_output = model(val_input)\n",
        "                    val_output = nn.Softmax(dim=1)(val_output)\n",
        "\n",
        "                    # Compute metrics for current iteration\n",
        "                    iteration_dice.append(dice_metric(y_pred = val_output, y = val_label).item())\n",
        "                    iou_metric(y_pred= val_output, y=val_label)\n",
        "\n",
        "            # Aggregate the final mean results\n",
        "            dice_score = torch.mean(torch.tensor(iteration_dice)).item()\n",
        "            mean_iou = iou_metric.aggregate().item()\n",
        "\n",
        "            # Reset the status for the next epoch\n",
        "            # dice_metric.reset()\n",
        "            iou_metric.reset()\n",
        "\n",
        "            dice_values.append(dice_score)\n",
        "            iou_values.append(mean_iou)\n",
        "\n",
        "            writer.add_scalar('Dice/val', dice_score, epoch)\n",
        "            writer.add_scalar('IoU/val', mean_iou, epoch)\n",
        "\n",
        "            if dice_score > best_dice:\n",
        "                best_dice = dice_score\n",
        "                best_metrics = (dice_score, mean_iou)\n",
        "                best_metric_epoch = epoch + 1\n",
        "                print(\"saved new best metric model!!!\")\n",
        "\n",
        "                torch.save(model.state_dict(), best_model_name)\n",
        "\n",
        "            print(\n",
        "                f\"current epoch: {epoch + 1},\"\n",
        "                f\" current mean dice: {dice_score:.4f},\"\n",
        "                f\" current mean iou: {mean_iou:.4f},\"\n",
        "                f\" best mean dice: {best_dice:.4f},\"\n",
        "                f\" at epoch: {best_metric_epoch}\"\n",
        "            )\n",
        "\n",
        "    print(\n",
        "        f\"train completed, metrics correspondic to best dice are: dice: {best_metrics[0]:.4f}, iou: {best_metrics[1]:.4f}\" #, acc: {best_metrics[2]:.4f}, rvd: {best_metrics[3]:.4f}\"\n",
        "        f\" at epoch: {best_metric_epoch}\"\n",
        "    )\n",
        "\n",
        "    with open(checkpoints_path + f'{general_name}_metrics_evolution.pkl', 'wb') as f:\n",
        "        pkl.dump((dice_values, iou_values, epoch_loss_values), f)\n",
        "\n",
        "    writer.close()\n",
        "\n",
        "    return best_model_name\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_model_name = train(device, net, model_name, len(msd_train_dataset), train_loader, val_loader, loss_function, optimizer, lr_scheduler, MAX_EPOCHS=100, VALIDATION_INTERVAL=2, EPOCH_OFFSET=12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_model_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get net weights\n",
        "net.state_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_model_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%reload_ext tensorboard\n",
        "\n",
        "%tensorboard --logdir=./pytorch_logging/unet_adam_lr3.00e-03_diceloss_epochs50/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### VALIDATION PROCEDURE ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "val_name = './checkpoints/UNET/min-max3_augmented_unet_nadam_lr5.00e-04_wd1.00e-03_diceloss_epoch2.pth'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "val_net = net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# TODO: Verificat dupa ce rezolv restul lucrurilor de ce nu merge loadul ca lumea\n",
        "val_net = nn.Sequential(nn.BatchNorm3d(1, affine=True), UNet(**UNet_metadata), nn.InstanceNorm3d(2, affine=True))\n",
        "val_net = nn.DataParallel(val_net, device_ids = [0, 1, 2, 3])\n",
        "val_net = val_net.to(device)\n",
        "val_net.load_state_dict(torch.load(val_name))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Same order test1: True\n",
            "No of skipped images: 48\n",
            "Index of image in batch:  0  ||  Index of image in dataset:  48\n",
            "Index of image in batch:  1  ||  Index of image in dataset:  49\n",
            "Index of image in batch:  2  ||  Index of image in dataset:  50\n",
            "Index of image in batch:  3  ||  Index of image in dataset:  51\n",
            "Index of image in batch:  4  ||  Index of image in dataset:  52\n",
            "Index of image in batch:  5  ||  Index of image in dataset:  53\n",
            "Index of image in batch:  6  ||  Index of image in dataset:  54\n",
            "Index of image in batch:  7  ||  Index of image in dataset:  55\n",
            "Index of image in batch:  8  ||  Index of image in dataset:  56\n",
            "Index of image in batch:  9  ||  Index of image in dataset:  57\n",
            "Index of image in batch:  10  ||  Index of image in dataset:  58\n",
            "Index of image in batch:  11  ||  Index of image in dataset:  59\n",
            "Index of image in batch:  12  ||  Index of image in dataset:  60\n",
            "Index of image in batch:  13  ||  Index of image in dataset:  61\n",
            "Index of image in batch:  14  ||  Index of image in dataset:  62\n",
            "Index of image in batch:  15  ||  Index of image in dataset:  63\n",
            "Same order test2: True\n"
          ]
        }
      ],
      "source": [
        "# IDIOT PROOF \"IT WORKS WELL\" TEST - VALIDATION LOADER #\n",
        "for val_data1 in val_loader:\n",
        "    val_input1, val_label1 = val_data1\n",
        "    break\n",
        "\n",
        "\n",
        "they_are_ordered = True\n",
        "\n",
        "for i in range(val_input1.shape[0]):\n",
        "    they_are_ordered &= (val_input1[i][0] == msd_val_dataset[i][0]).all()\n",
        "\n",
        "print(f\"Same order test1: {they_are_ordered}\")\n",
        "\n",
        "i = 3\n",
        "skipped_images = 0\n",
        "for val_data2 in val_loader:\n",
        "    if i > 0:\n",
        "        i -= 1\n",
        "        skipped_images += val_data2[0].shape[0]\n",
        "        continue\n",
        "    else:\n",
        "        val_input2, val_label2 = val_data2\n",
        "        break\n",
        "\n",
        "print(f\"No of skipped images: {skipped_images}\")\n",
        "for j in range(3*val_input2.shape[0], 4*val_input2.shape[0]):\n",
        "    print(\"Index of image in batch: \", j % val_input2.shape[0], \" || \", \"Index of image in dataset: \", j)\n",
        "    they_are_ordered &= (val_input2[j % val_input2.shape[0]][0] == msd_val_dataset[j][0]).all()\n",
        "\n",
        "print(f\"Same order test2: {they_are_ordered}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'lr_scheduler' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_355001/2596415222.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# IDIOT PROOF \"IT WORKS WELL\" TEST - LR SCHEDULER LOADER #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfuture_lr1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_last_lr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdecayRate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfuture_lr2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_last_lr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'lr_scheduler' is not defined"
          ]
        }
      ],
      "source": [
        "# IDIOT PROOF \"IT WORKS WELL\" TEST - LR SCHEDULER LOADER #\n",
        "future_lr1 = lr_scheduler.get_last_lr()[0] * decayRate\n",
        "\n",
        "lr_scheduler.step()\n",
        "future_lr2 = lr_scheduler.get_last_lr()[0]\n",
        "\n",
        "print(f\"LR works as expected: {future_lr1 == future_lr2}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "def hard_threshold_labels(labels, threshold = 0.5, cutoff_flag = False):\n",
        "    '''Thresholds the labels to 0 or 1 based on a specified threshold.'''\n",
        "\n",
        "    sigmoid_activation = Activations(sigmoid=True)\n",
        "\n",
        "    labels = sigmoid_activation(labels)\n",
        "\n",
        "\n",
        "    if cutoff_flag:\n",
        "        hard_thresholding = AsDiscrete(threshold=threshold)\n",
        "        mask_from_background = (1 - labels[1])\n",
        "        mask = labels[0]\n",
        "\n",
        "        # Mean based on the two masks\n",
        "        labels = (mask_from_background + mask) / 2\n",
        "        max_value = torch.max(labels)\n",
        "        min_value = torch.min(labels)\n",
        "\n",
        "        if max_value > 0:\n",
        "            labels = (labels - min_value) / (max_value - min_value)\n",
        "\n",
        "        labels = hard_thresholding(labels)\n",
        "        labels = np.expand_dims(labels, 0)\n",
        "        # print(labels.shape)\n",
        "\n",
        "    return labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_prediction_label_side_by_side(img, label, prediction, threshold = 0.5):\n",
        "    '''\n",
        "    Plots a 2 x 3 grid with the image, label and prediction side by side on the first row.\n",
        "\n",
        "    On the second row, the image is plotted with the label and prediction overlayed.\n",
        "    '''\n",
        "\n",
        "    for i in range(img.shape[0]):\n",
        "        print(i)\n",
        "        if i > 1:\n",
        "            break\n",
        "        im = img[i].to('cpu').detach().numpy()\n",
        "        target = label[i].to('cpu').detach().numpy()\n",
        "        output = prediction[i].to('cpu').detach().numpy()\n",
        "        # output = hard_threshold_labels(output, threshold, cutoff_flag = False)\n",
        "\n",
        "        for j in range(img.shape[-1]):\n",
        "            fig, ax = plt.subplots(2, 3, figsize=(15, 5))\n",
        "\n",
        "            ax[0, 0].imshow(im[0, ..., j], cmap = 'gray')\n",
        "            ax[0, 1].imshow(target[0, ..., j], cmap = 'jet')\n",
        "            im1 = ax[0, 2].imshow(output[1, ..., j], cmap = 'jet')\n",
        "            ax[1, 0].imshow(im[0, ..., j], cmap = 'gray')\n",
        "            ax[1, 1].imshow(im[0, ..., j], cmap = 'gray')\n",
        "            ax[1, 1].imshow(target[0, ..., j], cmap = 'jet', alpha = 0.5)\n",
        "            ax[1, 2].imshow(im[0, ..., j], cmap = 'gray')\n",
        "            im2 = ax[1, 2].imshow(output[1, ..., j], cmap = 'jet', alpha = 0.5)\n",
        "\n",
        "            plt.colorbar(im1, ax=ax[0, 2])\n",
        "            plt.colorbar(im2, ax=ax[1, 2])  \n",
        "\n",
        "            for k in range(2):\n",
        "                for l in range(3):\n",
        "                    ax[k, l].invert_yaxis()\n",
        "                    ax[k, l].set_axis_off()\n",
        "\n",
        "                    if k == 0:\n",
        "                        ax[k, l].set_title(['Image', 'Label', 'Prediction'][l])\n",
        "                    if i == 1:\n",
        "                        ax[k, l].set_title(['Image', 'Label overlay', 'Prediction overlay'][l])\n",
        "\n",
        "\n",
        "            # TODO: In alta zi, fa tight layout\n",
        "            fig.tight_layout()\n",
        "            plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "qualitative_plots_flag = False\n",
        "save_qualitative_plots_flag = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "### PERFORMANCE METRICS ###\n",
        "dice_metric_eval = DiceHelper(include_background = False, reduction = \"mean\", get_not_nans=False, ignore_empty=True) # include_background = False,\n",
        "iou_metric_eval = MeanIoU(include_background=False, reduction = \"mean\", get_not_nans=False, ignore_empty=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "if save_qualitative_plots_flag:\n",
        "    if not os.path.exists('./plots'):\n",
        "        os.mkdir('./plots')\n",
        "        os.mkdir(f'./plots/{model_name.upper()}')\n",
        "        os.mkdir(f'./plots/{model_name.upper()}/images')\n",
        "        os.mkdir(f'./plots/{model_name.upper()}/gifs')\n",
        "\n",
        "    if not os.path.exists(f'./plots/{model_name.upper()}'):\n",
        "        os.mkdir(f'./plots/{model_name.upper()}')\n",
        "        os.mkdir(f'./plots/{model_name.upper()}/images')\n",
        "        os.mkdir(f'./plots/{model_name.upper()}/gifs')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "def precision_score_(groundtruth_mask, pred_mask):\n",
        "    intersect = np.sum(pred_mask*groundtruth_mask)\n",
        "    total_pixel_pred = np.sum(pred_mask)\n",
        "    precision = np.mean(intersect/total_pixel_pred)\n",
        "    return round(precision, 3)\n",
        "\n",
        "def recall_score_(groundtruth_mask, pred_mask):\n",
        "    intersect = np.sum(pred_mask*groundtruth_mask)\n",
        "    total_pixel_truth = np.sum(groundtruth_mask)\n",
        "\n",
        "    if total_pixel_truth == 0:\n",
        "        if np.sum(pred_mask) == 0:\n",
        "            return 1\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "        \n",
        "    recall = np.mean(intersect/total_pixel_truth)\n",
        "    return round(recall, 3)\n",
        "\n",
        "def dice_coef(groundtruth_mask, pred_mask):\n",
        "    intersect = np.sum(pred_mask*groundtruth_mask)\n",
        "    total_sum = np.sum(pred_mask) + np.sum(groundtruth_mask)\n",
        "    dice = np.mean(2*intersect/total_sum)\n",
        "    return round(dice, 3) #round up to 3 decimal places\n",
        "\n",
        "def iou(groundtruth_mask, pred_mask):\n",
        "    intersect = np.sum(pred_mask*groundtruth_mask)\n",
        "    union = np.sum(pred_mask) + np.sum(groundtruth_mask) - intersect\n",
        "    iou = np.mean(intersect/union)\n",
        "    return round(iou, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current index: 0 / 546\n",
            "Current index: 1 / 546\n",
            "Current index: 2 / 546\n",
            "Current index: 3 / 546\n",
            "Current index: 4 / 546\n",
            "Current index: 5 / 546\n",
            "Current index: 6 / 546\n",
            "Current index: 7 / 546\n",
            "Current index: 8 / 546\n",
            "Current index: 9 / 546\n",
            "Current index: 10 / 546\n",
            "Current index: 11 / 546\n",
            "Current index: 12 / 546\n",
            "Current index: 13 / 546\n",
            "Current index: 14 / 546\n",
            "Current index: 15 / 546\n",
            "Current index: 16 / 546\n",
            "Current index: 17 / 546\n",
            "Current index: 18 / 546\n",
            "Current index: 19 / 546\n",
            "Current index: 20 / 546\n",
            "Current index: 21 / 546\n",
            "Current index: 22 / 546\n",
            "Current index: 23 / 546\n",
            "Current index: 24 / 546\n",
            "Current index: 25 / 546\n",
            "Current index: 26 / 546\n",
            "Current index: 27 / 546\n",
            "Current index: 28 / 546\n",
            "Current index: 29 / 546\n",
            "Current index: 30 / 546\n",
            "Current index: 31 / 546\n",
            "Current index: 32 / 546\n",
            "Current index: 33 / 546\n",
            "Current index: 34 / 546\n",
            "Current index: 35 / 546\n",
            "Current index: 36 / 546\n",
            "Current index: 37 / 546\n",
            "Current index: 38 / 546\n",
            "Current index: 39 / 546\n",
            "Current index: 40 / 546\n",
            "Current index: 41 / 546\n",
            "Current index: 42 / 546\n",
            "Current index: 43 / 546\n",
            "Current index: 44 / 546\n",
            "Current index: 45 / 546\n",
            "Current index: 46 / 546\n",
            "Current index: 47 / 546\n",
            "Current index: 48 / 546\n",
            "Current index: 49 / 546\n",
            "Current index: 50 / 546\n",
            "Current index: 51 / 546\n",
            "Current index: 52 / 546\n",
            "Current index: 53 / 546\n",
            "Current index: 54 / 546\n",
            "Current index: 55 / 546\n",
            "Current index: 56 / 546\n",
            "Current index: 57 / 546\n",
            "Current index: 58 / 546\n",
            "Current index: 59 / 546\n",
            "Current index: 60 / 546\n",
            "Current index: 61 / 546\n",
            "Current index: 62 / 546\n"
          ]
        },
        {
          "ename": "SystemExit",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "no_val_iters = 0\n",
        "loss_val = []\n",
        "dice_val = []\n",
        "iou_val = []\n",
        "precision_val = []\n",
        "recall_val = []\n",
        "\n",
        "post_pred = Compose([AsDiscrete(argmax=True, to_onehot=2)])\n",
        "post_label = Compose([AsDiscrete(to_onehot=2)])\n",
        "\n",
        "save_val_data = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    val_net.eval()\n",
        "\n",
        "    count_plots = 0\n",
        "    idx = 0\n",
        "\n",
        "    for batch in val_loader:\n",
        "        val_img, val_label = batch\n",
        "        val_img, val_label = val_img.to(device), val_label.to(device)\n",
        "\n",
        "        val_output = val_net(val_img)\n",
        "        val_output = nn.Softmax(dim=1)(val_output)\n",
        "\n",
        "        # roi_size = (512, 512, 2)\n",
        "        # sw_batch_size = 1\n",
        "        # val_outputs_dice = sliding_window_inference(val_img, roi_size, sw_batch_size, val_net)\n",
        "\n",
        "        # val_outputs_dice = []\n",
        "        # for val_crt_output in decollate_batch(val_outputs_dice):\n",
        "        #     print(val_crt_output == None)\n",
        "        #     print(val_crt_output.shape)\n",
        "        #     val_outputs_dice.append(post_pred(nn.Softmax(dim=1)(val_crt_output)))\n",
        "\n",
        "        # val_label_dice = []\n",
        "        # for crt_label in decollate_batch(val_label):\n",
        "        #     val_label_dice.append(post_label(crt_label))\n",
        "\n",
        "        # compute metric for current iteration\n",
        "        dice_val.append(dice_metric_eval(y_pred=val_output, y=val_label).item())\n",
        "        # dice_val.append(dice_coef(val_label[1].to('cpu').detach().numpy(), val_output[1].to('cpu').detach().numpy()))\n",
        "        # precision_val.append(precision_score_(val_label[1].to('cpu').detach().numpy(), val_output[1].to('cpu').detach().numpy()))\n",
        "        # recall_val.append(recall_score_(val_label[1].to('cpu').detach().numpy(), val_output[1].to('cpu').detach().numpy()))\n",
        "        # iou_val.append(iou(val_label[1].to('cpu').detach().numpy(), val_output[1].to('cpu').detach().numpy()))\n",
        "        iou_metric_eval(y_pred=val_output, y=val_label)\n",
        "        # print(\"Dice score : \", dice_metric_eval.aggregate().item())\n",
        "        # print(\"IoU score : \", iou_metric_eval.aggregate().item())\n",
        "        # print(\"Loss : \", loss_function(val_output, val_label).item())\n",
        "\n",
        "        # dice_val.append(dice_metric_eval.aggregate().item())\n",
        "        # loss_val.append(loss_function(val_output, val_label).item())\n",
        "        # iou_val.append(iou_metric_eval.aggregate().item())\n",
        "        # no_val_iters += 1\n",
        "\n",
        "\n",
        "        # if recall_val[-1] > 0.8:\n",
        "        #     save_val_data.append((val_img, val_label, val_output, recall_val[-1]))\n",
        "        \n",
        "\n",
        "        if qualitative_plots_flag:\n",
        "            plot_prediction_label_side_by_side(val_img, val_label, val_output, threshold = 0.5)\n",
        "            break\n",
        "\n",
        "        if save_qualitative_plots_flag:\n",
        "\n",
        "            for i in range(val_img.shape[0]):\n",
        "\n",
        "                if idx == len(msd_val_dataset):\n",
        "                    sys.exit()\n",
        "\n",
        "                print(f\"Current index: {idx} / {len(msd_val_dataset)}\")\n",
        "                # Don't save images for the padding indices\n",
        "                # Only iterate through the unique slices for each patient\n",
        "                patient_id, slices = msd_val_dataset.stacks_in_order_indices[idx]\n",
        "                slices = np.unique(slices)\n",
        "                \n",
        "                if patient_id > 0:\n",
        "                    sys.exit()\n",
        "\n",
        "                im = val_img[i].to('cpu').detach().numpy()\n",
        "                target = val_label[i].to('cpu').detach().numpy()\n",
        "                output = val_output[i].to('cpu').detach().numpy()\n",
        "\n",
        "                # Normalize the output to 0 or 1\n",
        "                # output = hard_threshold_labels(output)\n",
        "\n",
        "                idx += 1\n",
        "                for j in range(len(slices)):\n",
        "                    plt.imshow(im[0, ..., j], cmap = 'gray')\n",
        "                    plt.imshow(output[1, ..., j], cmap = 'jet', alpha = 0.5)\n",
        "                    plt.gca().invert_yaxis()\n",
        "                    plt.gca().set_axis_off()\n",
        "\n",
        "                    plt.savefig(f'./plots/{model_name.upper()}/images/patient{patient_id}_slice{slices[j]}.png')\n",
        "                    plt.close()\n",
        "\n",
        "    # Aggregate the final mean results\n",
        "    dice_score_eval = torch.mean(torch.tensor(dice_val)).item()\n",
        "    mean_iou_eval = iou_metric_eval.aggregate().item()\n",
        "    # mean_iou_eval = torch.mean(torch.tensor(iou_val)).item()\n",
        "    p_score_eval = np.mean(precision_val)\n",
        "    r_score_eval = np.mean(recall_val)\n",
        "\n",
        "\n",
        "    # # Reset the status\n",
        "    # dice_metric_eval.reset()\n",
        "    # iou_metric_eval.reset()\n",
        "\n",
        "    print(f\"Evaluation metrics: dice: {dice_score_eval:.4f}, iou: {mean_iou_eval:.4f}, precision: {p_score_eval:.4f}, recall: {r_score_eval:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "for (val_img, val_label, val_output, crt_recall) in save_val_data:\n",
        "    \n",
        "    print(f'Recall score: {crt_recall}')\n",
        "    for i in range(val_img.shape[-1]):\n",
        "        im = val_img[0].to('cpu').detach().numpy()\n",
        "        target = val_label[1].to('cpu').detach().numpy()\n",
        "        output = val_output[1].to('cpu').detach().numpy()\n",
        "\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "        axes[0].imshow(im[0, ..., i], cmap = 'gray')\n",
        "        axes[0].imshow(target[0, ..., i], cmap = 'jet', alpha = 0.5)\n",
        "\n",
        "        axes[1].imshow(im[0, ..., i], cmap = 'gray')\n",
        "        axes[1].imshow(output[1, ..., i], cmap = 'jet', alpha = 0.5)\n",
        "\n",
        "        for j in range(2):\n",
        "            axes[j].invert_yaxis()\n",
        "            axes[j].set_axis_off()\n",
        "            \n",
        "\n",
        "        plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(dice_val, label = 'Dice')\n",
        "plt.plot(iou_val, label = 'IoU')\n",
        "plt.plot(precision_val, label = 'Precision')\n",
        "plt.plot(recall_val, label = 'Recall')\n",
        "plt.plot(loss_val, label = 'Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "no_val_iters, len(dice_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# IDIOT PROOF \"IT WORKS WELL\" TEST - MONAI ONE HOT ENCODING MAP #\n",
        "\n",
        "from monai.networks.utils import one_hot\n",
        "# Search for a slice with a tumour\n",
        "\n",
        "for i in range(len(msd_val_dataset)):\n",
        "    if(msd_val_dataset[i][1].sum() > 3000):\n",
        "        single_label = msd_val_dataset[i][1].unsqueeze(0).to('cpu')\n",
        "        print(\"OK\")\n",
        "        break\n",
        "\n",
        "# See how the label looks like\n",
        "plt.imshow(single_label[0, 0, ..., 3].to('cpu').detach().numpy(), cmap = 'gray')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print(\"ONE HOT ENCODING TEST\")\n",
        "# See how the one-hot encoding looks like splitted in background and tumour\n",
        "one_hot_lbl = one_hot(single_label, 2)\n",
        "one_hot_lbl = one_hot_lbl.detach().cpu().numpy()\n",
        "\n",
        "plt.imshow(one_hot_lbl[0, 0, ..., 3], cmap = 'gray')\n",
        "plt.show()\n",
        "plt.imshow(one_hot_lbl[0, 1, ..., 3], cmap = 'gray')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### NUMBER OF TRAINABLE PARAMETERS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------------------------------------------------------------------------+------------+\n",
            "|                                  Modules                                   | Parameters |\n",
            "+----------------------------------------------------------------------------+------------+\n",
            "|                              module.0.weight                               |     1      |\n",
            "|                               module.0.bias                                |     1      |\n",
            "|                  module.1.model.0.conv.unit0.conv.weight                   |    2000    |\n",
            "|                   module.1.model.0.conv.unit0.conv.bias                    |     16     |\n",
            "|                  module.1.model.0.conv.unit0.adn.N.weight                  |     16     |\n",
            "|                   module.1.model.0.conv.unit0.adn.N.bias                   |     16     |\n",
            "|                  module.1.model.0.conv.unit0.adn.A.weight                  |     1      |\n",
            "|                  module.1.model.0.conv.unit1.conv.weight                   |   32000    |\n",
            "|                   module.1.model.0.conv.unit1.conv.bias                    |     16     |\n",
            "|                  module.1.model.0.conv.unit1.adn.N.weight                  |     16     |\n",
            "|                   module.1.model.0.conv.unit1.adn.N.bias                   |     16     |\n",
            "|                  module.1.model.0.conv.unit1.adn.A.weight                  |     1      |\n",
            "|                  module.1.model.0.conv.unit2.conv.weight                   |   32000    |\n",
            "|                   module.1.model.0.conv.unit2.conv.bias                    |     16     |\n",
            "|                  module.1.model.0.conv.unit2.adn.N.weight                  |     16     |\n",
            "|                   module.1.model.0.conv.unit2.adn.N.bias                   |     16     |\n",
            "|                  module.1.model.0.conv.unit2.adn.A.weight                  |     1      |\n",
            "|                  module.1.model.0.conv.unit3.conv.weight                   |   32000    |\n",
            "|                   module.1.model.0.conv.unit3.conv.bias                    |     16     |\n",
            "|                  module.1.model.0.conv.unit3.adn.N.weight                  |     16     |\n",
            "|                   module.1.model.0.conv.unit3.adn.N.bias                   |     16     |\n",
            "|                  module.1.model.0.conv.unit3.adn.A.weight                  |     1      |\n",
            "|                      module.1.model.0.residual.weight                      |     16     |\n",
            "|                       module.1.model.0.residual.bias                       |     16     |\n",
            "|            module.1.model.1.submodule.0.conv.unit0.conv.weight             |   64000    |\n",
            "|             module.1.model.1.submodule.0.conv.unit0.conv.bias              |     32     |\n",
            "|            module.1.model.1.submodule.0.conv.unit0.adn.N.weight            |     32     |\n",
            "|             module.1.model.1.submodule.0.conv.unit0.adn.N.bias             |     32     |\n",
            "|            module.1.model.1.submodule.0.conv.unit0.adn.A.weight            |     1      |\n",
            "|            module.1.model.1.submodule.0.conv.unit1.conv.weight             |   128000   |\n",
            "|             module.1.model.1.submodule.0.conv.unit1.conv.bias              |     32     |\n",
            "|            module.1.model.1.submodule.0.conv.unit1.adn.N.weight            |     32     |\n",
            "|             module.1.model.1.submodule.0.conv.unit1.adn.N.bias             |     32     |\n",
            "|            module.1.model.1.submodule.0.conv.unit1.adn.A.weight            |     1      |\n",
            "|            module.1.model.1.submodule.0.conv.unit2.conv.weight             |   128000   |\n",
            "|             module.1.model.1.submodule.0.conv.unit2.conv.bias              |     32     |\n",
            "|            module.1.model.1.submodule.0.conv.unit2.adn.N.weight            |     32     |\n",
            "|             module.1.model.1.submodule.0.conv.unit2.adn.N.bias             |     32     |\n",
            "|            module.1.model.1.submodule.0.conv.unit2.adn.A.weight            |     1      |\n",
            "|            module.1.model.1.submodule.0.conv.unit3.conv.weight             |   128000   |\n",
            "|             module.1.model.1.submodule.0.conv.unit3.conv.bias              |     32     |\n",
            "|            module.1.model.1.submodule.0.conv.unit3.adn.N.weight            |     32     |\n",
            "|             module.1.model.1.submodule.0.conv.unit3.adn.N.bias             |     32     |\n",
            "|            module.1.model.1.submodule.0.conv.unit3.adn.A.weight            |     1      |\n",
            "|                module.1.model.1.submodule.0.residual.weight                |   64000    |\n",
            "|                 module.1.model.1.submodule.0.residual.bias                 |     32     |\n",
            "|      module.1.model.1.submodule.1.submodule.0.conv.unit0.conv.weight       |   128000   |\n",
            "|       module.1.model.1.submodule.1.submodule.0.conv.unit0.conv.bias        |     32     |\n",
            "|      module.1.model.1.submodule.1.submodule.0.conv.unit0.adn.N.weight      |     32     |\n",
            "|       module.1.model.1.submodule.1.submodule.0.conv.unit0.adn.N.bias       |     32     |\n",
            "|      module.1.model.1.submodule.1.submodule.0.conv.unit0.adn.A.weight      |     1      |\n",
            "|      module.1.model.1.submodule.1.submodule.0.conv.unit1.conv.weight       |   128000   |\n",
            "|       module.1.model.1.submodule.1.submodule.0.conv.unit1.conv.bias        |     32     |\n",
            "|      module.1.model.1.submodule.1.submodule.0.conv.unit1.adn.N.weight      |     32     |\n",
            "|       module.1.model.1.submodule.1.submodule.0.conv.unit1.adn.N.bias       |     32     |\n",
            "|      module.1.model.1.submodule.1.submodule.0.conv.unit1.adn.A.weight      |     1      |\n",
            "|      module.1.model.1.submodule.1.submodule.0.conv.unit2.conv.weight       |   128000   |\n",
            "|       module.1.model.1.submodule.1.submodule.0.conv.unit2.conv.bias        |     32     |\n",
            "|      module.1.model.1.submodule.1.submodule.0.conv.unit2.adn.N.weight      |     32     |\n",
            "|       module.1.model.1.submodule.1.submodule.0.conv.unit2.adn.N.bias       |     32     |\n",
            "|      module.1.model.1.submodule.1.submodule.0.conv.unit2.adn.A.weight      |     1      |\n",
            "|      module.1.model.1.submodule.1.submodule.0.conv.unit3.conv.weight       |   128000   |\n",
            "|       module.1.model.1.submodule.1.submodule.0.conv.unit3.conv.bias        |     32     |\n",
            "|      module.1.model.1.submodule.1.submodule.0.conv.unit3.adn.N.weight      |     32     |\n",
            "|       module.1.model.1.submodule.1.submodule.0.conv.unit3.adn.N.bias       |     32     |\n",
            "|      module.1.model.1.submodule.1.submodule.0.conv.unit3.adn.A.weight      |     1      |\n",
            "| module.1.model.1.submodule.1.submodule.1.submodule.conv.unit0.conv.weight  |   512000   |\n",
            "|  module.1.model.1.submodule.1.submodule.1.submodule.conv.unit0.conv.bias   |    128     |\n",
            "| module.1.model.1.submodule.1.submodule.1.submodule.conv.unit0.adn.N.weight |    128     |\n",
            "|  module.1.model.1.submodule.1.submodule.1.submodule.conv.unit0.adn.N.bias  |    128     |\n",
            "| module.1.model.1.submodule.1.submodule.1.submodule.conv.unit0.adn.A.weight |     1      |\n",
            "| module.1.model.1.submodule.1.submodule.1.submodule.conv.unit1.conv.weight  |  2048000   |\n",
            "|  module.1.model.1.submodule.1.submodule.1.submodule.conv.unit1.conv.bias   |    128     |\n",
            "| module.1.model.1.submodule.1.submodule.1.submodule.conv.unit1.adn.N.weight |    128     |\n",
            "|  module.1.model.1.submodule.1.submodule.1.submodule.conv.unit1.adn.N.bias  |    128     |\n",
            "| module.1.model.1.submodule.1.submodule.1.submodule.conv.unit1.adn.A.weight |     1      |\n",
            "| module.1.model.1.submodule.1.submodule.1.submodule.conv.unit2.conv.weight  |  2048000   |\n",
            "|  module.1.model.1.submodule.1.submodule.1.submodule.conv.unit2.conv.bias   |    128     |\n",
            "| module.1.model.1.submodule.1.submodule.1.submodule.conv.unit2.adn.N.weight |    128     |\n",
            "|  module.1.model.1.submodule.1.submodule.1.submodule.conv.unit2.adn.N.bias  |    128     |\n",
            "| module.1.model.1.submodule.1.submodule.1.submodule.conv.unit2.adn.A.weight |     1      |\n",
            "| module.1.model.1.submodule.1.submodule.1.submodule.conv.unit3.conv.weight  |  2048000   |\n",
            "|  module.1.model.1.submodule.1.submodule.1.submodule.conv.unit3.conv.bias   |    128     |\n",
            "| module.1.model.1.submodule.1.submodule.1.submodule.conv.unit3.adn.N.weight |    128     |\n",
            "|  module.1.model.1.submodule.1.submodule.1.submodule.conv.unit3.adn.N.bias  |    128     |\n",
            "| module.1.model.1.submodule.1.submodule.1.submodule.conv.unit3.adn.A.weight |     1      |\n",
            "|     module.1.model.1.submodule.1.submodule.1.submodule.residual.weight     |    4096    |\n",
            "|      module.1.model.1.submodule.1.submodule.1.submodule.residual.bias      |    128     |\n",
            "|           module.1.model.1.submodule.1.submodule.2.0.conv.weight           |   138240   |\n",
            "|            module.1.model.1.submodule.1.submodule.2.0.conv.bias            |     32     |\n",
            "|          module.1.model.1.submodule.1.submodule.2.0.adn.N.weight           |     32     |\n",
            "|           module.1.model.1.submodule.1.submodule.2.0.adn.N.bias            |     32     |\n",
            "|          module.1.model.1.submodule.1.submodule.2.0.adn.A.weight           |     1      |\n",
            "|     module.1.model.1.submodule.1.submodule.2.1.conv.unit0.conv.weight      |   128000   |\n",
            "|      module.1.model.1.submodule.1.submodule.2.1.conv.unit0.conv.bias       |     32     |\n",
            "|     module.1.model.1.submodule.1.submodule.2.1.conv.unit0.adn.N.weight     |     32     |\n",
            "|      module.1.model.1.submodule.1.submodule.2.1.conv.unit0.adn.N.bias      |     32     |\n",
            "|     module.1.model.1.submodule.1.submodule.2.1.conv.unit0.adn.A.weight     |     1      |\n",
            "|                 module.1.model.1.submodule.2.0.conv.weight                 |   27648    |\n",
            "|                  module.1.model.1.submodule.2.0.conv.bias                  |     16     |\n",
            "|                module.1.model.1.submodule.2.0.adn.N.weight                 |     16     |\n",
            "|                 module.1.model.1.submodule.2.0.adn.N.bias                  |     16     |\n",
            "|                module.1.model.1.submodule.2.0.adn.A.weight                 |     1      |\n",
            "|           module.1.model.1.submodule.2.1.conv.unit0.conv.weight            |   32000    |\n",
            "|            module.1.model.1.submodule.2.1.conv.unit0.conv.bias             |     16     |\n",
            "|           module.1.model.1.submodule.2.1.conv.unit0.adn.N.weight           |     16     |\n",
            "|            module.1.model.1.submodule.2.1.conv.unit0.adn.N.bias            |     16     |\n",
            "|           module.1.model.1.submodule.2.1.conv.unit0.adn.A.weight           |     1      |\n",
            "|                       module.1.model.2.0.conv.weight                       |    1728    |\n",
            "|                        module.1.model.2.0.conv.bias                        |     2      |\n",
            "|                      module.1.model.2.0.adn.N.weight                       |     2      |\n",
            "|                       module.1.model.2.0.adn.N.bias                        |     2      |\n",
            "|                      module.1.model.2.0.adn.A.weight                       |     1      |\n",
            "|                 module.1.model.2.1.conv.unit0.conv.weight                  |    500     |\n",
            "|                  module.1.model.2.1.conv.unit0.conv.bias                   |     2      |\n",
            "|                              module.2.weight                               |     2      |\n",
            "|                               module.2.bias                                |     2      |\n",
            "+----------------------------------------------------------------------------+------------+\n",
            "Total Trainable Params: 8.11 M params\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "8113223"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from prettytable import PrettyTable\n",
        "\n",
        "def count_parameters(model):\n",
        "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
        "    total_params = 0\n",
        "    for name, parameter in model.named_parameters():\n",
        "        if not parameter.requires_grad:\n",
        "            continue\n",
        "        params = parameter.numel()\n",
        "        table.add_row([name, params])\n",
        "        total_params += params\n",
        "    print(table)\n",
        "    # Get the total number of parameters as millions\n",
        "    print(f\"Total Trainable Params: {total_params/1e6:.2f} M params\")\n",
        "    return total_params\n",
        "    \n",
        "count_parameters(net)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
