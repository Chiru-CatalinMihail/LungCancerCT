{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TODO DOCUMENTAT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### IMPORTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\") # remove some scikit-image warnings\n",
        "\n",
        "# import monai\n",
        "# # monai.config.print_config()\n",
        "\n",
        "from monai.apps import DecathlonDataset\n",
        "from monai.data import DataLoader, CacheDataset, decollate_batch\n",
        "# # from monai.data import decollate_patient_batch\n",
        "# from monai.utils import first, set_determinism\n",
        "from monai.networks.nets import UNet, DynUNet, AttentionUnet, ViTAutoEnc, UNETR\n",
        "from monai.networks.layers import Norm\n",
        "from monai.metrics import DiceMetric, MeanIoU, compute_average_surface_distance\n",
        "from monai.losses import DiceLoss\n",
        "from monai.inferers import sliding_window_inference\n",
        "from monai.transforms import (\n",
        "    LoadImage,\n",
        "    LoadImageD,\n",
        "    EnsureChannelFirstD,\n",
        "#     AddChannelD,\n",
        "    ScaleIntensityD,\n",
        "    ToTensorD,\n",
        "    Compose,\n",
        "    AsDiscreteD,\n",
        "    SpacingD,\n",
        "    OrientationD,\n",
        "    ResizeD,\n",
        "    RandAffineD,\n",
        "    AsDiscrete,\n",
        "    AsDiscreted,\n",
        "    EnsureTyped,\n",
        "    EnsureType,\n",
        "    LoadImageD,\n",
        "    EnsureChannelFirstD,\n",
        "    OrientationD,\n",
        "    SpacingD,\n",
        "    ScaleIntensity,\n",
        "    ResizeD,\n",
        "    RandAffineD,\n",
        "    RandFlipD,\n",
        "    RandRotateD,\n",
        "    RandZoomD,\n",
        "#     RandDeformD,\n",
        "    ToTensorD,\n",
        ")\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader as TorchDataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import cv2\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import pickle as pkl\n",
        "\n",
        "from torch.utils.data.dataloader import default_collate\n",
        "\n",
        "from hyperparams import *\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### HYPERPARAMS ###\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [],
      "source": [
        "### HYPERPARAMS ###\n",
        "crt_dir = os.getcwd()\n",
        "datasets_path = f'/raid/CataChiru/MedicalDecathlonTensors/'\n",
        "model_name = 'unet'\n",
        "checkpoints_path = f'{crt_dir}/checkpoints/{model_name.upper()}/'\n",
        "\n",
        "DEBUG_MODE = True\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### PREPROCESSING TRANSFORMS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [],
      "source": [
        "# train_transform1 = Compose([\n",
        "#     LoadImageD(keys=KEYS),\n",
        "#     EnsureChannelFirstD(keys=KEYS),\n",
        "#     OrientationD(keys=KEYS, axcodes='RAS'),\n",
        "#     # SpacingD(keys=KEYS, pixdim=(1., 1., 1.), mode=('bilinear', 'nearest')),\n",
        "#     # ScaleIntensityD(keys=\"image\"),\n",
        "#     # ResizeD(keys=KEYS, spatial_size=(IMG_HEIGHT, IMG_HEIGHT, NO_STACKED_IMGS), mode=('trilinear', 'nearest')),\n",
        "#     # # ResizeD(keys=KEYS, spatial_size=(128, 128, 64), mode=('trilinear', 'nearest')),\n",
        "\n",
        "#     # RandAffineD(\n",
        "#     #     keys=KEYS,\n",
        "#     #     spatial_size= (IMG_HEIGHT, IMG_HEIGHT, NO_STACKED_IMGS),\n",
        "\n",
        "#     #     # spatial_size=(128, 128, 64),\n",
        "#     #     rotate_range=(0, 0, np.pi/12),\n",
        "#     #     scale_range=(0.1, 0.1, 0.1),\n",
        "#     #     mode=('bilinear', 'nearest'),\n",
        "#     #     prob=0.5\n",
        "#     # ),\n",
        "#     # RandFlipD(keys=KEYS, spatial_axis=[0,1], prob=0.5),\n",
        "#     # RandRotateD(keys=KEYS, range_x=np.pi/12, range_y=np.pi/12, range_z=np.pi/12, prob=0.5),\n",
        "#     # RandZoomD(keys=KEYS, min_zoom=0.9, max_zoom=1.1, prob=0.5),\n",
        "#     ToTensorD(keys=KEYS),\n",
        "# ])\n",
        "\n",
        "# train_transform2 = Compose([\n",
        "#     LoadImageD(keys=KEYS),\n",
        "#     EnsureChannelFirstD(keys=KEYS),\n",
        "#     OrientationD(keys=KEYS, axcodes='RAS'),\n",
        "# ])\n",
        "\n",
        "# val_transform = Compose([\n",
        "#     LoadImageD(keys = KEYS),\n",
        "#     EnsureChannelFirstD(keys = KEYS),\n",
        "#     OrientationD(KEYS, axcodes='RAS'),\n",
        "#     SpacingD(keys = KEYS, pixdim = (1., 1., 1.), mode = ('bilinear', 'nearest')),\n",
        "#     ScaleIntensityD(keys = \"image\"),\n",
        "#     ResizeD(KEYS, (IMG_HEIGHT, IMG_HEIGHT, NO_STACKED_IMGS), mode=('trilinear', 'nearest')),\n",
        "#     ToTensorD(KEYS),\n",
        "# ])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### MAIN ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You are using cuda\n",
            "Number of images in a stack: 64\n"
          ]
        }
      ],
      "source": [
        "# Initialize torch and cuda\n",
        "cuda = torch.cuda.is_available()\n",
        "\n",
        "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
        "num_workers = 4 if cuda else 1\n",
        "\n",
        "print(f'You are using {device}')\n",
        "\n",
        "print(f'Number of images in a stack: {NO_STACKED_IMGS}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.multiprocessing.set_start_method('spawn', force=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set random seed\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "random.seed(0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DATA LOADER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_tumour_percentage_per_patient(tumour):\n",
        "    '''\n",
        "    Computes the percentage of tumour in each patient\n",
        "    '''\n",
        "\n",
        "    return 100*np.sum(tumour) / np.prod(tumour.shape)\n",
        "\n",
        "def build_stack_ordered_indices(dataset_type : str, stack_size : int, tumours : list = None, upload_flag : bool = True ) -> list[tuple]:\n",
        "    ''' Iterates an overlapping sliding window of stack_size images for each patient per batch.\n",
        "\n",
        "    For dataset_type = 'training', the stacks are with an overlapping window from start to finish.\n",
        "    Returns a list of tuples (patient_id, stack_indices, tumour_percentage, has_tumour, original_idx) \n",
        "    \n",
        "    For dataset_type = 'validation', the stacks are built with a stride of \"stack_size\" over the entire volume of each patient, and the last stack is padded with the last slice of the volume up to \"stack_size\" slices.\n",
        "    Returns a list of tuples (patient_id, stack_indices, tumour_percentage, has_tumour, original_idx) \n",
        "\n",
        "\n",
        "    '''\n",
        "    # Load the number of slices for each patient for each dataset type\n",
        "    with open(f'./slices_per_patient_{dataset_type}.pkl', 'rb') as f:\n",
        "        slices_per_patient = pkl.load(f)\n",
        "\n",
        "    stacks_in_order_indices =[]\n",
        "    if dataset_type == \"training\":\n",
        "        real_idx = 0\n",
        "        # Saves the indices of the sliding window for each patient\n",
        "        for patient_id, slices in tqdm(enumerate(slices_per_patient)):\n",
        "            crt_slices = []\n",
        "\n",
        "            for i in range(0, slices - stack_size + 1):\n",
        "                    stacks_range = np.arange(i, i+stack_size)\n",
        "                    crt_tumour = tumours[patient_id][1][..., i:i+stack_size]\n",
        "                    tumour_percentage = compute_tumour_percentage_per_patient(crt_tumour)\n",
        "\n",
        "                    crt_slices.append((patient_id, stacks_range, tumour_percentage, tumour_percentage > 0, real_idx))\n",
        "\n",
        "                    real_idx += 1\n",
        "            \n",
        "            \n",
        "            stacks_in_order_indices += crt_slices\n",
        "\n",
        "    elif dataset_type == 'validation':\n",
        "\n",
        "        for patient_id, slices in enumerate(slices_per_patient):\n",
        "            # Non-overlapping sliding window of stack_size images, with stride = stack_size\n",
        "\n",
        "            padding = stack_size - slices % stack_size\n",
        "            remaining_difference = -1\n",
        "\n",
        "            for i in range(0, slices, stack_size):\n",
        "                remaining_difference = i + stack_size - slices\n",
        "                if remaining_difference > 0:\n",
        "                    break\n",
        "\n",
        "                stacks_in_order_indices.append((patient_id, np.arange(i, i+stack_size)))\n",
        "\n",
        "            if padding  % stack_size != 0 and remaining_difference > 0:\n",
        "                remaining_slices_indices = np.arange(i, slices)\n",
        "                repeated_slices = np.repeat(slices - 1, padding)\n",
        "                batch_indices = np.hstack((remaining_slices_indices, repeated_slices))\n",
        "                stacks_in_order_indices.append((patient_id, batch_indices))\n",
        "\n",
        "            # If the last stack is smaller than stack_size, we pad it with the last slice of the volume\n",
        "\n",
        "    if upload_flag and not os.path.exists(f'./ordered_indices_{dataset_type}_stack={stack_size}.pkl'):\n",
        "        with open(f'./ordered_indices_{dataset_type}_stack={stack_size}.pkl', 'wb') as f:\n",
        "            pkl.dump(stacks_in_order_indices, f)\n",
        "\n",
        "    return stacks_in_order_indices\n",
        "\n",
        "\n",
        "def create_oversampled_index_dataset(ordered_stacks, dataset_type, stack_size, print_flag = False, tumour_percent_threshold : float = 0.5, tumorous_proportion : float = 0.7, upload_flag : bool = True):\n",
        "    '''\n",
        "    Based on the threshold set for the tumour percentage, splits the dataset and oversamples the desired portion of the dataset\n",
        "    Returns the complete list of indices by intercalating the two portions\n",
        "    '''\n",
        "\n",
        "    ordered_stacks.sort(key = lambda x: x[2], reverse = True)\n",
        "\n",
        "    small_tumour_stacks = list(filter(lambda x: x[2] < tumour_percent_threshold, ordered_stacks))\n",
        "    length_small_tumour_stacks = len(small_tumour_stacks)\n",
        "\n",
        "\n",
        "    big_tumour_stacks = list(filter(lambda x: x[2] > tumour_percent_threshold, ordered_stacks))\n",
        "    length_big_tumour_stacks = len(big_tumour_stacks)\n",
        "    big_tumour_stacks = big_tumour_stacks * int(length_small_tumour_stacks / ((1-tumorous_proportion) * length_big_tumour_stacks))\n",
        "\n",
        "    print(\"Small tumour stacks\", length_small_tumour_stacks)\n",
        "    print(\"Big tumour stacks\", length_big_tumour_stacks)\n",
        "\n",
        "    length = length_small_tumour_stacks + len(big_tumour_stacks)\n",
        "\n",
        "    all_stacks = []\n",
        "\n",
        "    small_tumour_idx = 0\n",
        "    big_tumour_idx = 0\n",
        "    \n",
        "    for i in range(length):\n",
        "        if i % 10 >= 7 and small_tumour_idx < length_small_tumour_stacks:\n",
        "            all_stacks.append(small_tumour_stacks[small_tumour_idx])\n",
        "            small_tumour_idx += 1\n",
        "\n",
        "            if print_flag:\n",
        "                print(\"Small\", i, small_tumour_idx)\n",
        "        else:\n",
        "            all_stacks.append(big_tumour_stacks[big_tumour_idx])\n",
        "            big_tumour_idx += 1\n",
        "            if print_flag:\n",
        "                print(\"Big\", i, big_tumour_idx)\n",
        "\n",
        "    if upload_flag and not os.path.exists(f'./{dataset_type}_indices_stack={stack_size}.pkl'):\n",
        "        with open(f'./{dataset_type}_indices_stack=6.pkl', 'wb') as f:\n",
        "            pkl.dump(all_stacks, f)\n",
        "\n",
        "    print(\"Length of the dataset\", len(all_stacks))\n",
        "    return all_stacks\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "def convert_labels_to_one_hot(labels: torch.Tensor, num_classes: int) -> torch.Tensor:\n",
        "    ''' Converts a tensor of labels to a one-hot tensor in which each channel corresponds to a binary decision for each class from the original tensor.'''\n",
        "    one_hot = torch.zeros((2*labels.shape[0], labels.shape[1], labels.shape[2], labels.shape[3])).to(labels.device)\n",
        "    \n",
        "    one_hot[0, :, :, :] = (labels == 0).squeeze(1).float()\n",
        "    one_hot[1, :, :, :] = (labels != 0).squeeze(1).float()\n",
        "\n",
        "    return one_hot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [],
      "source": [
        "class msdDataset(Dataset):\n",
        "    def __init__(self, dataset_folder, transform = None, stack_size = 6, device = 'cpu'):\n",
        "        '''Am stabilit stack_size la 6 pe baza discutiei cu expertii care iau intre 3 si 6 imagini in stack + EDA2'''\n",
        "        self.img_folder = dataset_folder + \"images/\"\n",
        "        self.label_folder = dataset_folder + \"labels/\"\n",
        "        self.no_patients = len(os.listdir(self.img_folder))\n",
        "        self.stack_size = stack_size\n",
        "        self.transform = transform\n",
        "\n",
        "        dataset_type = 'training' if 'training' in dataset_folder else 'validation'\n",
        "        print(dataset_type)\n",
        "        self.patients = [self.get_img_and_label(i) for i in range(self.no_patients)]\n",
        "\n",
        "        # If the indices for the dataset have been already built, load them, otherwise build them\n",
        "        if os.path.exists(f'./ordered_{dataset_type}_indices_stack={stack_size}.pkl'):\n",
        "            print(f'./ordered_{dataset_type}_indices_stack={stack_size}.pkl exists. Loading the ordered indices.')\n",
        "            with open(f'./ordered_{dataset_type}_indices_stack={stack_size}.pkl', 'rb') as f:\n",
        "                self.stacks_in_order_indices = pkl.load(f)\n",
        "        else:\n",
        "            print(f'./{dataset_type}_indices_stack={stack_size}.pkl does not exist. Building the ordered indices.')\n",
        "            self.stacks_in_order_indices = build_stack_ordered_indices(dataset_type, stack_size, self.patients)\n",
        "            \n",
        "        # If the dataset is the validation one, we need to know where are the padding indices for each patient\n",
        "        if dataset_type == 'validation':\n",
        "\n",
        "            # TODO: CHANGE TO A DICTIONARY WITH FORBIDDEN SLICES/INDICES (IDX : SKIP)\n",
        "            self.val_padding_indices = self.validation_redundant_slices()\n",
        "        else:\n",
        "            # TBD:\n",
        "            if os.path.exists(f'./{dataset_type}_indices_stack={stack_size}.pkl'):\n",
        "                print(f'./{dataset_type}_indices_stack={stack_size}.pkl exists. Loading the overall indices.')\n",
        "                with open(f'./{dataset_type}_indices_stack={stack_size}.pkl', 'rb') as f:\n",
        "                    self.stacks_in_order_indices = pkl.load(f)\n",
        "            else:\n",
        "                print(f'./{dataset_type}_indices_stack={stack_size}.pkl does not exist. Building the overall indices.')\n",
        "                self.stacks_in_order_indices = create_oversampled_index_dataset(self.stacks_in_order_indices, dataset_type, stack_size)\n",
        "        \n",
        "\n",
        "        # Even though stacks_in_order is a list of different sizes tuple between train and validation, we extract patient_id as the first element of each of those tuples\n",
        "        self.idx_to_patient = {idx: stack_tuple[0] for idx, stack_tuple in enumerate(self.stacks_in_order_indices)}\n",
        "\n",
        "        # self.device = device\n",
        "        self.length = len(self.stacks_in_order_indices)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def get_img_and_label(self, patient_id):\n",
        "        ''' Helper function: For a specified patient returns its image and label stacks from the dataset '''\n",
        "\n",
        "        img = torch.load(self.img_folder + f'patient_{patient_id}.pt')\n",
        "        label = torch.load(self.label_folder + f'patient_{patient_id}.pt')\n",
        "        return img, label\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # print(f'Getting item {idx}/{len(self)}')\n",
        "        # print(self.img_folder + f'patient_{patient_id}.pt')\n",
        "\n",
        "        # Based on current index, get the patient_id and the slices that form the current stack\n",
        "\n",
        "        if idx >= 0 and idx < self.length:\n",
        "            stacks_tuple = self.stacks_in_order_indices[idx]\n",
        "            patient_id, chosen_stacks = stacks_tuple[0], stacks_tuple[1]\n",
        "\n",
        "            img, label = self.patients[patient_id]\n",
        "            # Filters the current stack of images and labels for the current batch\n",
        "            img, label = img[..., chosen_stacks], label[..., chosen_stacks]\n",
        "\n",
        "            # TODO: Add transforms and debug for them\n",
        "            if self.transform:\n",
        "                img = self.transform(img)\n",
        "                label = self.transform(label)\n",
        "                print(type(img), type(label))\n",
        "\n",
        "            # label = convert_labels_to_one_hot(label, 2) # Not needed for now, as MONAI handles the conversion internally\n",
        "            return img, label\n",
        "        else:\n",
        "            raise IndexError\n",
        "\n",
        "\n",
        "    def validation_redundant_slices(self):\n",
        "        '''When creating batches for validation to match the structure of the training batches,\n",
        "        we pad the last batch with the last slice, until it reaches the same size as the others.\n",
        "\n",
        "        This function returns the number of redundant slices for each patient in the validation set.\n",
        "        '''\n",
        "        \n",
        "        with open(f'./slices_per_patient_validation.pkl', 'rb') as f:\n",
        "            slices_per_patient = pkl.load(f)\n",
        "\n",
        "\n",
        "        self.val_padding_indices = []\n",
        "        maxes = [slices - 1 for slices in slices_per_patient]\n",
        "        for patient_id, slices in self.stacks_in_order_indices:\n",
        "            # print(slices, maxes[patient_id])\n",
        "            if slices[-1] != maxes[patient_id]:\n",
        "                continue\n",
        "            else:\n",
        "                self.val_padding_indices.append(list(slices).count(maxes[patient_id]) - 1)\n",
        "        return self.val_padding_indices\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_name = \"training/\"\n",
        "val_name = \"validation/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/raid/CataChiru/MedicalDecathlonTensors/'"
            ]
          },
          "execution_count": 105,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "datasets_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training\n",
            "./training_indices_stack=6.pkl does not exist. Building the ordered indices.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "51it [01:32,  1.81s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "./training_indices_stack=6.pkl exists. Loading the overall indices.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "msd_train_dataset = msdDataset(datasets_path + train_name, transform = None, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "validation\n",
            "./validation_indices_stack=6.pkl does not exist. Building the ordered indices.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[3, 3, 0, 2, 2, 1, 0, 3, 3, 0, 5, 0]"
            ]
          },
          "execution_count": 108,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "msd_val_dataset = msdDataset(datasets_path + val_name, transform = None, device=device)\n",
        "msd_val_dataset.val_padding_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [],
      "source": [
        "# msd_train_dataset.indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [],
      "source": [
        "# msd_val_dataset.indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(59573, 546)"
            ]
          },
          "execution_count": 109,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(msd_train_dataset), len(msd_val_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_loader = TorchDataLoader(msd_train_dataset, batch_size = BATCH_SIZE, shuffle = False, num_workers = num_workers, collate_fn=lambda x: tuple(x_ for x_ in default_collate(x))) # TODO: Comment collate_fn if it doesn't work\n",
        "val_loader = TorchDataLoader(msd_val_dataset, batch_size = BATCH_SIZE, shuffle = False, num_workers = num_workers) #, collate_fn=lambda x: tuple(x_.to(device) for x_ in default_collate(x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <module>\n",
            "  File \"/home/aimas/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n",
            "    exitcode = _main(fd, parent_sentinel)\n",
            "  File \"/home/aimas/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 126, in _main\n",
            "    self = reduction.pickle.load(from_parent)\n",
            "AttributeError: Can't get attribute 'msdDataset' on <module '__main__' (built-in)>\n",
            "  0%|          | 0/14894 [04:40<?, ?it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_5611/535594828.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mdebug_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1195\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1196\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    436\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_worker_number_rationality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_MultiProcessingDataLoaderIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1037\u001b[0m             \u001b[0;31m#     before it starts, and __del__ tries to join but will get:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m             \u001b[0;31m#     AssertionError: can only join a started process.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m             \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index_queues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_queue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.9/multiprocessing/process.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    119\u001b[0m                \u001b[0;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.9/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mDefaultContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.9/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpopen_spawn_posix\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0;32mclass\u001b[0m \u001b[0mForkServerProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseProcess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mduplicate_for_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinalizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_launch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mduplicate_for_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\u001b[0m in \u001b[0;36m_launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentinel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparent_r\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosefd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                 \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetbuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mfds_to_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "for batch in tqdm(train_loader):\n",
        "    debug_img, debug_label = batch\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "debug_img.shape, debug_label.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_channels = debug_img.shape[1]\n",
        "output_channels = 2*input_channels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "debug_img, debug_label = debug_img.to(device), debug_label.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "a = debug_img[0].to('cpu').detach().numpy()\n",
        "b = debug_label[0].to('cpu').detach().numpy()\n",
        "for i in range(debug_img.shape[-1]):\n",
        "    plt.imshow(a[0, ..., i], cmap = 'gray')\n",
        "    plt.imshow(b[0, ..., i], cmap = 'jet', alpha = 0.5)\n",
        "    plt.gca().invert_yaxis()\n",
        "    # plt.gca().set_aspect('equal', adjustable='box')\n",
        "    plt.gca().set_axis_off()\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MODEL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### MODEL HYPERPARAMS ###\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# UNet_metadata = dict(\n",
        "#     spatial_dims = 3,\n",
        "#     in_channels = 1,\n",
        "#     out_channels = 2,\n",
        "#     channels = (64, 128, 256, 512),\n",
        "#     strides = (2, 2, 2),\n",
        "#     num_res_units = 2,\n",
        "#     norm = Norm.BATCH,\n",
        "#     # act = torch.nn.ReLU,\n",
        "#     dropout = 0.1\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Instantiate model\n",
        "# model = UNet(**UNet_metadata).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "net = nn.Sequential(\n",
        "    nn.Conv3d(in_channels=1, out_channels=2, kernel_size=3, padding = 1),\n",
        "    nn.Conv3d(in_channels=2, out_channels=2, kernel_size=3, padding = 1, stride=2),\n",
        "    nn.PReLU(),\n",
        "    nn.Softmax(dim = 1)\n",
        ").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "net(debug_img.to(device)).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def trim_tensors(x1, x2):\n",
        "    '''Trim the tensors to the same size on Height, Width and Depth.'''\n",
        "    # Trim on Height, Width and Depth to match dimensions\n",
        "    min_shapes = [min(x1.shape[i], x2.shape[i]) for i in range(2, 5)]\n",
        "\n",
        "    x1 = x1[..., :min_shapes[0], :min_shapes[1], :min_shapes[2]]\n",
        "    x2 = x2[..., :min_shapes[0], :min_shapes[1], :min_shapes[2]]\n",
        "\n",
        "    return x1, x2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DoubleConvo(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, no_filters = None):\n",
        "\n",
        "        super(DoubleConvo, self).__init__()\n",
        "\n",
        "        if not no_filters:\n",
        "            no_filters = 2 * in_channels\n",
        "\n",
        "        # print(f\"Entering encoder: in{in_channels}, out{out_channels}\")\n",
        "\n",
        "        self.conv1 = nn.Conv3d(in_channels, no_filters, kernel_size=(3,3,3), padding = (1,1,1), stride = (1,1,1))\n",
        "        self.conv2 = nn.Conv3d(no_filters, out_channels, kernel_size=(3,3,3), padding = (1,1,1), stride = (1,1,1))\n",
        "        self.batch_norm1 = nn.BatchNorm3d(no_filters)\n",
        "        self.batch_norm2 = nn.BatchNorm3d(out_channels)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.batch_norm1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.batch_norm2(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def to(self, device):\n",
        "        self.conv1.to(device)\n",
        "        self.conv2.to(device)\n",
        "        self.batch_norm1.to(device)\n",
        "        self.batch_norm2.to(device)\n",
        "        return self\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DoubleConvo(1, 2).to(device)(debug_img).shape, debug_img.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EncoderUnit_3D(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, no_filters = None):\n",
        "\n",
        "        super(EncoderUnit_3D, self).__init__()\n",
        "\n",
        "        if not no_filters:\n",
        "            no_filters = out_channels\n",
        "\n",
        "        # print(f\"Entering encoder: in{in_channels}, out{out_channels}\")\n",
        "\n",
        "        self.conv1 = nn.Conv3d(in_channels, no_filters, kernel_size=(3,3,1), padding = (1,1,0), stride = (2,2,1))\n",
        "        self.conv2 = nn.Conv3d(no_filters, out_channels, kernel_size=3, padding = 1)\n",
        "\n",
        "        # TODO: Determine if DoubleConvo is useful\n",
        "        self.double_convo = DoubleConvo(out_channels, out_channels)\n",
        "        self.instance_norm = nn.InstanceNorm3d(out_channels)\n",
        "        self.prelu = nn.PReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_enc = self.conv1(x)\n",
        "        # print(\"After convolution+stride: \", x_enc.shape)\n",
        "        x_enc = self.instance_norm(x_enc)\n",
        "        x_enc = self.prelu(x_enc)\n",
        "        # print(\"AIC\")\n",
        "        x_enc = self.conv2(x_enc)\n",
        "        # print(\"After convolution2 \", x_enc.shape)\n",
        "        # print(\"MUERTO\")\n",
        "        x_enc = self.instance_norm(x_enc)\n",
        "        x_enc = self.prelu(x_enc)\n",
        "        x_enc = self.double_convo(x_enc)\n",
        "        x_enc = self.instance_norm(x_enc)\n",
        "        x_enc = self.prelu(x_enc)\n",
        "\n",
        "        # x, x_enc = trim_tensors(x, x_enc)\n",
        "        # x_res = torch.cat([x, x_enc], dim = 1)\n",
        "\n",
        "        # x_enc = x_enc.repeat((1,1,2,2,1))\n",
        "        # print(x.shape, x_enc.shape)\n",
        "        x_res = x.repeat((1,2,1,1,1)) + x_enc.repeat((1,1,2,2,1))\n",
        "        # print(x.shape, x_res.shape)\n",
        "        return x_res, x_enc\n",
        "    \n",
        "    def to(self, device):\n",
        "        self.conv1.to(device)\n",
        "        self.conv2.to(device)\n",
        "        self.double_convo.to(device)\n",
        "        self.instance_norm.to(device)\n",
        "        self.prelu.to(device)\n",
        "        return self"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DecoderUnit_3D(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, no_filters = None):\n",
        "\n",
        "        super(DecoderUnit_3D, self).__init__()\n",
        "\n",
        "        if not no_filters:\n",
        "            no_filters = out_channels\n",
        "\n",
        "        # print(f\"Entering decoder: in{in_channels}, out{out_channels}\")\n",
        "\n",
        "\n",
        "        self.conv1_t = nn.ConvTranspose3d(in_channels, out_channels, kernel_size=(2,2,1), stride = (2,2,1))\n",
        "        self.conv2 = nn.Conv3d(no_filters, out_channels, kernel_size=3, padding = 1)\n",
        "\n",
        "        # TODO: Determine if DoubleConvo is useful\n",
        "        self.double_convo = DoubleConvo(out_channels, out_channels)\n",
        "\n",
        "        self.instance_norm = nn.InstanceNorm3d(out_channels)\n",
        "        self.prelu = nn.PReLU()\n",
        "\n",
        "    def forward(self, x_res, x_enc):\n",
        "        # print(x_res.shape, x_enc.shape)\n",
        "        x_enc = self.conv1_t(x_enc)\n",
        "        # print(\"After transposed convolution: \", x_res.shape, x_enc.shape)\n",
        "\n",
        "        # x_res, x_enc = trim_tensors(x_res, x_enc)\n",
        "        # x = torch.cat([x_res, x_enc], dim = 1)\n",
        "\n",
        "        x = x_res + x_enc\n",
        "\n",
        "        # print(x.shape)\n",
        "        x = self.conv2(x)\n",
        "        x = self.instance_norm(x)\n",
        "        x = self.prelu(x)\n",
        "        x = self.double_convo(x)\n",
        "        x = self.instance_norm(x)\n",
        "        x = self.prelu(x)\n",
        "        return x\n",
        "    \n",
        "    def to(self, device):\n",
        "        self.conv1_t.to(device)\n",
        "        self.conv2.to(device)\n",
        "        self.double_convo.to(device)\n",
        "        self.instance_norm.to(device)\n",
        "        self.prelu.to(device)\n",
        "        return self"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "net = EncoderUnit_3D(1, 2).to(device)\n",
        "x_res, x_enc = net(debug_img)\n",
        "print(\"Encoder 1:\", x_res.shape, x_enc.shape)\n",
        "\n",
        "net2 = EncoderUnit_3D(2, 4).to(device)\n",
        "x2, x_enc2 = net2(x_enc.to(device))\n",
        "print(\"Encoder 2:\", x2.shape, x_enc2.shape)\n",
        "\n",
        "bottom_layer = nn.Conv3d(in_channels=4, out_channels=8, kernel_size=(3,3,1), padding = (1,1,0), stride = (1,1,1)).to(device)\n",
        "x_enc3 = bottom_layer(x_enc2)\n",
        "print(\"Bottom layer:\", x_enc3.shape)\n",
        "\n",
        "net3 = DecoderUnit_3D(8, 4).to(device)\n",
        "print(\"Decoder 1:\", x2.shape, x_enc3.shape)\n",
        "x_dec1 = net3(x2, x_enc3)\n",
        "print(x_dec1.shape)\n",
        "\n",
        "net4 = DecoderUnit_3D(4, 2).to(device)\n",
        "print(\"Decoder 2:\", x2.shape, x_enc3.shape)\n",
        "x = net4(debug_img, x_dec1)\n",
        "print(x.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Iterator, Tuple\n",
        "\n",
        "\n",
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "\n",
        "class UNet_3D(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, depth = 2, no_filters=None):\n",
        "        super(UNet_3D, self).__init__()\n",
        "\n",
        "        if not no_filters:\n",
        "            no_filters = 2\n",
        "\n",
        "        input_sizes = [no_filters**i for i in range(depth+2)]\n",
        "\n",
        "        self.encoders = []\n",
        "        self.decoders = []\n",
        "\n",
        "        for i in range(depth):\n",
        "            self.encoders.append((EncoderUnit_3D(input_sizes[i], input_sizes[i+1]), input_sizes[i+1]))\n",
        "            self.decoders.append((DecoderUnit_3D(input_sizes[depth-i+1], input_sizes[depth-i]), input_sizes[depth-i-1]))\n",
        "\n",
        "\n",
        "        self.bottom_layer = nn.Conv3d(in_channels=input_sizes[depth], out_channels=input_sizes[depth+1], kernel_size=(3,3,1), padding = (1,1,0), stride = (1,1,1))\n",
        "        self.double_convo = DoubleConvo(input_sizes[depth+1], input_sizes[depth+1])\n",
        "        self.softmax = nn.Softmax(dim = 1)\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        features = []\n",
        "        \n",
        "        x_enc = x\n",
        "        for encoder, _ in self.encoders:\n",
        "            # print(x_enc.shape)\n",
        "            x_res, x_enc  = encoder(x_enc)\n",
        "            # print(\"Result after encoding: \", x_res.shape, x_enc.shape)\n",
        "            \n",
        "            features.append(x_res)\n",
        "\n",
        "        x_enc = self.bottom_layer(x_enc)\n",
        "        x_enc = self.double_convo(x_enc)\n",
        "        # print(\"Result after bottom layer: \", x_enc.shape)\n",
        "\n",
        "\n",
        "        for decoder, _ in self.decoders:\n",
        "            # print(\"Result before decoding: \", x_res.shape, x_enc.shape)\n",
        "            x_res = features.pop()\n",
        "            x_enc = decoder(x_res, x_enc)\n",
        "            # print(\"Result after decoding: \", x_res.shape, x_enc.shape)\n",
        "\n",
        "\n",
        "        x_enc = self.softmax(x_enc)\n",
        "        return x_enc\n",
        "\n",
        "    def to(self, device):\n",
        "        for encoder, _ in self.encoders:\n",
        "            encoder.to(device)\n",
        "        for decoder, _ in self.decoders:\n",
        "            decoder.to(device)\n",
        "        self.bottom_layer.to(device)\n",
        "        self.double_convo.to(device)\n",
        "        return self\n",
        "    \n",
        "    def save(self, path):\n",
        "        torch.save(self.state_dict(), path)\n",
        "        print(f'Model saved to {path}')\n",
        "\n",
        "    def named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, Parameter]]:\n",
        "        params = []\n",
        "        for i, (encoder, _) in enumerate(self.encoders):\n",
        "            params += list(encoder.named_parameters(prefix + f'encoder_{i}_'))\n",
        "        \n",
        "        for i, (decoder, _) in enumerate(self.decoders):\n",
        "            params += list(decoder.named_parameters(prefix + f'decoder_{i}_'))\n",
        "\n",
        "        params += list(self.bottom_layer.named_parameters(prefix + 'bottom_layer_'))\n",
        "        params += list(self.double_convo.named_parameters(prefix + 'bottom_layer_double_convo_'))\n",
        "        # params += super().named_parameters(prefix, recurse, remove_duplicate)\n",
        "        return params\n",
        "\n",
        "    @staticmethod\n",
        "    def load(path, in_channels, out_channels, device):\n",
        "        model = UNet_3D(in_channels, out_channels).to(device)\n",
        "        model.load_state_dict(torch.load(path))\n",
        "        return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "net = UNet_3D(in_channels=input_channels, out_channels=output_channels).to(device)\n",
        "net(debug_img).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### LOSS ###\n",
        "loss_functions = {\n",
        "    'dice': DiceLoss(to_onehot_y = True, softmax = True, include_background=False),\n",
        "    'cross_entropy': nn.CrossEntropyLoss(),\n",
        "    'custom': nn.BCELoss()\n",
        "}\n",
        "\n",
        "loss_key = 'dice'\n",
        "\n",
        "loss_function = loss_functions[loss_key]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LEARNING_RATE = 1e-4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "optimizers = { 'adam' : torch.optim.Adam, 'sgd' : torch.optim.SGD, 'nadam' : torch.optim.NAdam, 'rmsprop' : torch.optim.RMSprop, 'adamw' : torch.optim.AdamW}\n",
        "optimizer_key = 'adam'\n",
        "\n",
        "# Instantiate optimizer\n",
        "optimizer = optimizers[optimizer_key](net.parameters(), lr = LEARNING_RATE)\n",
        "\n",
        "# Instantiate learning rate scheduler\n",
        "decayRate = 0.999\n",
        "lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decayRate)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### PERFORMANCE METRICS ###\n",
        "dice_metric = DiceMetric(include_background = False, reduction = \"mean\") # include_background = False,\n",
        "iou_metric = MeanIoU(include_background=False, reduction = \"mean\", get_not_nans=False, ignore_empty=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TRAINING PROCEDURE ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(device, model, model_name, train_dataset, train_loader, val_loader, loss_function, optimizer, lr_scheduler, MAX_EPOCHS=20, VALIDATION_INTERVAL=2):\n",
        "\n",
        "    # Variables to get the best model\n",
        "    best_dice = -1\n",
        "    best_metrics = None\n",
        "    best_metric_epoch = -1\n",
        "\n",
        "\n",
        "    general_name = f'{model_name}_{optimizer_key}_lr{LEARNING_RATE:.2e}_{loss_key}loss'\n",
        "    best_model_name = checkpoints_path + f'{general_name}_best.pth'\n",
        "    writer = SummaryWriter(log_dir=f\"./pytorch_logging/{general_name}_epochs{MAX_EPOCHS}\")\n",
        "\n",
        "\n",
        "    # Evaluation metrics per epoch\n",
        "    dice_values = []\n",
        "    iou_values = []\n",
        "\n",
        "    epoch_loss_values = []\n",
        "\n",
        "    for epoch in range(1, MAX_EPOCHS+1):\n",
        "        print(\"-\" * 12)\n",
        "        print(f\"Epoch {epoch}/{MAX_EPOCHS}\")\n",
        "\n",
        "        # Turn model to \"train\" mode\n",
        "        model.train()\n",
        "\n",
        "        epoch_loss = 0\n",
        "        for step, batch_data in enumerate(train_loader):\n",
        "            step += 1\n",
        "\n",
        "            train_input, label = batch_data\n",
        "            train_input, label = train_input.to(device), label.to(device)\n",
        "\n",
        "\n",
        "            # # A common pytorch Deep Learning format to train model\n",
        "            # optimizer.zero_grad()\n",
        "            output = model(train_input)\n",
        "\n",
        "            loss = loss_function(output, label)\n",
        "            loss.backward() # Compute gradient\n",
        "            optimizer.step() # Update model's parameters\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            print(f\"{step}/{len(train_dataset) // train_loader.batch_size}, \"\n",
        "                f\"train_loss: {loss.item():.4f}\")\n",
        "\n",
        "        epoch_loss /= step\n",
        "        epoch_loss_values.append(epoch_loss)\n",
        "        writer.add_scalar('Loss/train', epoch_loss, epoch)\n",
        "\n",
        "        print(f\"epoch {epoch} average loss: {epoch_loss:.4f}\")\n",
        "\n",
        "        if epoch % VALIDATION_INTERVAL == 0:\n",
        "            # Save current checkpoint of the network\n",
        "\n",
        "            print(f\"Saving checkpoint: {epoch//VALIDATION_INTERVAL + 1} / {MAX_EPOCHS//VALIDATION_INTERVAL}!!!\")\n",
        "            name = checkpoints_path + f'{general_name}_epoch{epoch}.pth'\n",
        "            model.save(name)\n",
        "\n",
        "            # Decay learning rate\n",
        "            lr_scheduler.step()\n",
        "\n",
        "            # Turn model to \"eval\" mode\n",
        "            model.eval()\n",
        "\n",
        "            # Disabling gradient calculation is useful for inference, when you are sure that you will not call Tensor.backward().\n",
        "            # It will reduce memory consumption for computations that would otherwise have requires_grad=True\n",
        "            with torch.no_grad():\n",
        "                iteration_ious = []\n",
        "                iteration_pixel_accuracies = []\n",
        "                iteration_rvds = []\n",
        "\n",
        "                for val_data in val_loader:\n",
        "                    val_input, val_label = val_data\n",
        "                    val_input, val_label = val_input.to(device), val_label.to(device)\n",
        "\n",
        "                    val_output = model(val_input)\n",
        "\n",
        "                    # Compute metrics for current iteration\n",
        "                    dice_metric(y_pred = val_output, y = val_label)\n",
        "                    iou_metric(y_pred=val_output, y=val_label)\n",
        "\n",
        "            # Aggregate the final mean results\n",
        "            dice_score = dice_metric.aggregate().item()\n",
        "            mean_iou = iou_metric.aggregate().item()\n",
        "\n",
        "            # Reset the status for the next epoch\n",
        "            dice_metric.reset()\n",
        "            iou_metric.reset()\n",
        "\n",
        "            dice_values.append(dice_score)\n",
        "            iou_values.append(mean_iou)\n",
        "\n",
        "            writer.add_scalar('Dice/val', dice_score, epoch)\n",
        "            writer.add_scalar('IoU/val', mean_iou, epoch)\n",
        "\n",
        "            if dice_score > best_dice:\n",
        "                best_dice = dice_score\n",
        "                best_metrics = (dice_score, mean_iou)\n",
        "                best_metric_epoch = epoch + 1\n",
        "                print(\"saved new best metric model!!!\")\n",
        "\n",
        "                model.save(best_model_name)\n",
        "\n",
        "            print(\n",
        "                f\"current epoch: {epoch + 1},\"\n",
        "                f\" current mean dice: {dice_score:.4f},\"\n",
        "                f\" current mean iou: {mean_iou:.4f},\"\n",
        "                f\" best mean dice: {best_dice:.4f},\"\n",
        "                f\" at epoch: {best_metric_epoch}\"\n",
        "            )\n",
        "\n",
        "    print(\n",
        "        f\"train completed, metrics correspondic to best dice are: dice: {best_metrics[0]:.4f}, iou: {best_metrics[1]:.4f}\" #, acc: {best_metrics[2]:.4f}, rvd: {best_metrics[3]:.4f}\"\n",
        "        f\" at epoch: {best_metric_epoch}\"\n",
        "    )\n",
        "\n",
        "    with open(checkpoints_path + f'{general_name}_metrics_evolution.pkl', 'wb') as f:\n",
        "        pkl.dump((dice_values, iou_values, epoch_loss_values), f)\n",
        "\n",
        "    writer.close()\n",
        "\n",
        "    return best_model_name\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_model_name = train(device, net, model_name, msd_train_dataset, train_loader, val_loader, loss_function, optimizer, lr_scheduler, MAX_EPOCHS=20, VALIDATION_INTERVAL=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%reload_ext tensorboard\n",
        "\n",
        "%tensorboard --logdir=./pytorch_logging/unet_adam_lr3.00e-03_diceloss_epochs50/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### VALIDATION PROCEDURE ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "val_net = UNet_3D.load(best_model_name, input_channels, output_channels, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# IDIOT PROOF \"IT WORKS WELL\" TEST - VALIDATION LOADER #\n",
        "for val_data1 in val_loader:\n",
        "    val_input1, val_label1 = val_data1\n",
        "    break\n",
        "\n",
        "\n",
        "they_are_ordered = True\n",
        "\n",
        "for i in range(val_input1.shape[0]):\n",
        "    they_are_ordered &= (val_input1[i][0] == msd_val_dataset[i][0]).all()\n",
        "\n",
        "print(f\"Same order test1: {they_are_ordered}\")\n",
        "\n",
        "i = 3\n",
        "skipped_images = 0\n",
        "for val_data2 in val_loader:\n",
        "    if i > 0:\n",
        "        i -= 1\n",
        "        skipped_images += val_data2[0].shape[0]\n",
        "        continue\n",
        "    else:\n",
        "        val_input2, val_label2 = val_data2\n",
        "        break\n",
        "\n",
        "print(f\"No of skipped images: {skipped_images}\")\n",
        "for j in range(3*val_input2.shape[0], 4*val_input2.shape[0]):\n",
        "    print(\"Index of image in batch: \", j % val_input2.shape[0], \" || \", \"Index of image in dataset: \", j)\n",
        "    they_are_ordered &= (val_input2[j % val_input2.shape[0]][0] == msd_val_dataset[j][0]).all()\n",
        "\n",
        "print(f\"Same order test2: {they_are_ordered}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# IDIOT PROOF \"IT WORKS WELL\" TEST - LR SCHEDULER LOADER #\n",
        "# future_lr1 = lr_scheduler.get_last_lr()[0] * decayRate\n",
        "\n",
        "# lr_scheduler.step()\n",
        "# future_lr2 = lr_scheduler.get_last_lr()[0]\n",
        "\n",
        "print(f\"LR works as expected: {future_lr1 == future_lr2}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def hard_threshold_labels(labels, threshold = 0.5):\n",
        "    '''Thresholds the labels to 0 or 1 based on a specified threshold.'''\n",
        "\n",
        "    mask_from_background = 1 - labels[0] > labels[1]\n",
        "    mask = labels[1] > threshold\n",
        "\n",
        "    # Mean based on the two masks\n",
        "    labels = (mask_from_background + mask) / 2\n",
        "    labels = np.expand_dims(labels, 0)\n",
        "    # print(labels.shape)\n",
        "    return labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_prediction_label_side_by_side(img, label, prediction, threshold = 0.5):\n",
        "    '''\n",
        "    Plots a 2 x 3 grid with the image, label and prediction side by side on the first row.\n",
        "\n",
        "    On the second row, the image is plotted with the label and prediction overlayed.\n",
        "    '''\n",
        "\n",
        "    for i in range(val_img.shape[0]):\n",
        "        im = img[i].to('cpu').detach().numpy()\n",
        "        target = label[i].to('cpu').detach().numpy()\n",
        "        output = prediction[i].to('cpu').detach().numpy()\n",
        "        output = hard_threshold_labels(output, threshold)\n",
        "\n",
        "        for j in range(img.shape[-1]):\n",
        "            fig, ax = plt.subplots(2, 3, figsize=(15, 5))\n",
        "\n",
        "            ax[0, 0].imshow(im[0, ..., j], cmap = 'gray')\n",
        "            ax[0, 1].imshow(target[0, ..., j], cmap = 'jet')\n",
        "            ax[0, 2].imshow(output[0, ..., j], cmap = 'jet')\n",
        "            ax[1, 0].imshow(im[0, ..., j], cmap = 'gray')\n",
        "            ax[1, 1].imshow(im[0, ..., j], cmap = 'gray')\n",
        "            ax[1, 1].imshow(target[0, ..., j], cmap = 'jet', alpha = 0.5)\n",
        "            ax[1, 2].imshow(im[0, ..., j], cmap = 'gray')\n",
        "            ax[1, 2].imshow(output[0, ..., j], cmap = 'jet', alpha = 0.5)\n",
        "\n",
        "            for i in range(2):\n",
        "                for j in range(3):\n",
        "                    ax[i, j].invert_yaxis()\n",
        "                    ax[i, j].set_axis_off()\n",
        "\n",
        "                    if i == 0:\n",
        "                        ax[i, j].set_title(['Image', 'Label', 'Prediction'][j])\n",
        "                    if i == 1:\n",
        "                        ax[i, j].set_title(['Image', 'Label overlay', 'Prediction overlay'][j])\n",
        "\n",
        "\n",
        "            # TODO: In alta zi, fa tight layout\n",
        "            fig.tight_layout()\n",
        "            plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "qualitative_plots_flag = False\n",
        "save_qualitative_plots_flag = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    val_net.eval()\n",
        "\n",
        "    previous_patient = -1\n",
        "    idx = 0\n",
        "    for batch in val_loader:\n",
        "        val_img, val_label = batch\n",
        "        val_img, val_label = val_img.to(device), val_label.to(device)\n",
        "\n",
        "        val_output = val_net(val_img)\n",
        "\n",
        "        if qualitative_plots_flag:\n",
        "            plot_prediction_label_side_by_side(val_img, val_label, val_output)\n",
        "\n",
        "\n",
        "\n",
        "        # TODO: Revazut aici pentru ca nu trece prin cate stack-uri ar trebui\n",
        "        if save_qualitative_plots_flag:\n",
        "            patient_id = msd_val_dataset.idx_to_patient[idx]\n",
        "            if patient_id != previous_patient:\n",
        "                idx = 0\n",
        "                previous_patient = patient_id\n",
        "\n",
        "            if patient_id > 1:\n",
        "                break\n",
        "\n",
        "            for i in range(val_img.shape[0]):\n",
        "                im = val_img[i].to('cpu').detach().numpy()\n",
        "                target = val_label[i].to('cpu').detach().numpy()\n",
        "                output = val_output[i].to('cpu').detach().numpy()\n",
        "                output = hard_threshold_labels(output)\n",
        "\n",
        "                for j in range(val_img.shape[-1]):\n",
        "                    plt.imshow(im[0, ..., j], cmap = 'gray')\n",
        "                    plt.imshow(output[0, ..., j], cmap = 'jet', alpha = 0.5)\n",
        "                    plt.gca().invert_yaxis()\n",
        "                    plt.gca().set_axis_off()\n",
        "\n",
        "                    plt.savefig(f'./plots/{model_name.upper()}/images/patient{patient_id}_slice{idx + j}.png')\n",
        "                    plt.close()\n",
        "                    idx += 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # break\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### NUMBER OF TRAINABLE PARAMETERS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from prettytable import PrettyTable\n",
        "\n",
        "def count_parameters(model):\n",
        "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
        "    total_params = 0\n",
        "    for name, parameter in model.named_parameters():\n",
        "        if not parameter.requires_grad:\n",
        "            continue\n",
        "        params = parameter.numel()\n",
        "        table.add_row([name, params])\n",
        "        total_params += params\n",
        "    print(table)\n",
        "    print(f\"Total Trainable Params: {total_params}\")\n",
        "    return total_params\n",
        "    \n",
        "count_parameters(net)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
